{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/mmcv/utils/registry.py:250: UserWarning: The old API of register_module(module, force=False) is deprecated and will be removed, please use the new API register_module(name=None, force=False, module=None) instead.\n",
      "  'The old API of register_module(module, force=False) '\n"
     ]
    }
   ],
   "source": [
    "import mmcv\n",
    "from mmcv import Config\n",
    "from mmdet.datasets import (build_dataloader, build_dataset,\n",
    "                            replace_ImageToTensor)\n",
    "from mmdet.models import build_detector\n",
    "from mmdet.apis import single_gpu_test\n",
    "from mmcv.runner import load_checkpoint\n",
    "import os\n",
    "from mmcv.parallel import MMDataParallel\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "from pycocotools.coco import COCO\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config:\n",
      "dataset_type = 'CocoDataset'\n",
      "data_root = 'data/coco/'\n",
      "img_norm_cfg = dict(\n",
      "    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)\n",
      "train_pipeline = [\n",
      "    dict(type='LoadImageFromFile'),\n",
      "    dict(type='LoadAnnotations', with_bbox=True),\n",
      "    dict(\n",
      "        type='Resize',\n",
      "        img_scale=[(1333, 480), (1333, 960)],\n",
      "        multiscale_mode='range',\n",
      "        keep_ratio=True),\n",
      "    dict(type='RandomFlip', flip_ratio=0.5),\n",
      "    dict(\n",
      "        type='Normalize',\n",
      "        mean=[123.675, 116.28, 103.53],\n",
      "        std=[58.395, 57.12, 57.375],\n",
      "        to_rgb=True),\n",
      "    dict(type='Pad', size_divisor=32),\n",
      "    dict(type='DefaultFormatBundle'),\n",
      "    dict(type='Collect', keys=['img', 'gt_bboxes', 'gt_labels'])\n",
      "]\n",
      "test_pipeline = [\n",
      "    dict(type='LoadImageFromFile'),\n",
      "    dict(\n",
      "        type='MultiScaleFlipAug',\n",
      "        img_scale=[(1333, 480), (1333, 800), (1333, 960)],\n",
      "        flip=False,\n",
      "        transforms=[\n",
      "            dict(type='Resize', keep_ratio=True),\n",
      "            dict(type='RandomFlip'),\n",
      "            dict(\n",
      "                type='Normalize',\n",
      "                mean=[123.675, 116.28, 103.53],\n",
      "                std=[58.395, 57.12, 57.375],\n",
      "                to_rgb=True),\n",
      "            dict(type='Pad', size_divisor=32),\n",
      "            dict(type='ImageToTensor', keys=['img']),\n",
      "            dict(type='Collect', keys=['img'])\n",
      "        ])\n",
      "]\n",
      "data = dict(\n",
      "    samples_per_gpu=8,\n",
      "    workers_per_gpu=4,\n",
      "    train=dict(\n",
      "        type='CocoDataset',\n",
      "        ann_file='../../input/data/train.json',\n",
      "        img_prefix='../../input/data/',\n",
      "        pipeline=[\n",
      "            dict(type='LoadImageFromFile'),\n",
      "            dict(type='LoadAnnotations', with_bbox=True),\n",
      "            dict(\n",
      "                type='Resize',\n",
      "                img_scale=[(1333, 480), (1333, 960)],\n",
      "                multiscale_mode='range',\n",
      "                keep_ratio=True),\n",
      "            dict(type='RandomFlip', flip_ratio=0.5),\n",
      "            dict(\n",
      "                type='Normalize',\n",
      "                mean=[123.675, 116.28, 103.53],\n",
      "                std=[58.395, 57.12, 57.375],\n",
      "                to_rgb=True),\n",
      "            dict(type='Pad', size_divisor=32),\n",
      "            dict(type='DefaultFormatBundle'),\n",
      "            dict(type='Collect', keys=['img', 'gt_bboxes', 'gt_labels'])\n",
      "        ],\n",
      "        classes=('UNKNOWN', 'General trash', 'Paper', 'Paper pack', 'Metal',\n",
      "                 'Glass', 'Plastic', 'Styrofoam', 'Plastic bag', 'Battery',\n",
      "                 'Clothing')),\n",
      "    val=dict(\n",
      "        type='CocoDataset',\n",
      "        ann_file='../../input/data/val.json',\n",
      "        img_prefix='../../input/data/',\n",
      "        pipeline=[\n",
      "            dict(type='LoadImageFromFile'),\n",
      "            dict(\n",
      "                type='MultiScaleFlipAug',\n",
      "                img_scale=(1333, 800),\n",
      "                flip=False,\n",
      "                transforms=[\n",
      "                    dict(type='Resize', keep_ratio=True),\n",
      "                    dict(type='RandomFlip'),\n",
      "                    dict(\n",
      "                        type='Normalize',\n",
      "                        mean=[123.675, 116.28, 103.53],\n",
      "                        std=[58.395, 57.12, 57.375],\n",
      "                        to_rgb=True),\n",
      "                    dict(type='Pad', size_divisor=32),\n",
      "                    dict(type='ImageToTensor', keys=['img']),\n",
      "                    dict(type='Collect', keys=['img'])\n",
      "                ])\n",
      "        ],\n",
      "        classes=('UNKNOWN', 'General trash', 'Paper', 'Paper pack', 'Metal',\n",
      "                 'Glass', 'Plastic', 'Styrofoam', 'Plastic bag', 'Battery',\n",
      "                 'Clothing')),\n",
      "    test=dict(\n",
      "        type='CocoDataset',\n",
      "        ann_file='../../input/data/test.json',\n",
      "        img_prefix='../../input/data/',\n",
      "        pipeline=[\n",
      "            dict(type='LoadImageFromFile'),\n",
      "            dict(\n",
      "                type='MultiScaleFlipAug',\n",
      "                img_scale=[(1333, 480), (1333, 800), (1333, 960)],\n",
      "                flip=False,\n",
      "                transforms=[\n",
      "                    dict(type='Resize', keep_ratio=True),\n",
      "                    dict(type='RandomFlip'),\n",
      "                    dict(\n",
      "                        type='Normalize',\n",
      "                        mean=[123.675, 116.28, 103.53],\n",
      "                        std=[58.395, 57.12, 57.375],\n",
      "                        to_rgb=True),\n",
      "                    dict(type='Pad', size_divisor=32),\n",
      "                    dict(type='ImageToTensor', keys=['img']),\n",
      "                    dict(type='Collect', keys=['img'])\n",
      "                ])\n",
      "        ],\n",
      "        classes=('UNKNOWN', 'General trash', 'Paper', 'Paper pack', 'Metal',\n",
      "                 'Glass', 'Plastic', 'Styrofoam', 'Plastic bag', 'Battery',\n",
      "                 'Clothing')))\n",
      "evaluation = dict(interval=1, metric='bbox')\n",
      "optimizer = dict(type='SGD', lr=0.01, momentum=0.9, weight_decay=0.0001)\n",
      "optimizer_config = dict(grad_clip=dict(max_norm=35, norm_type=2))\n",
      "lr_config = dict(\n",
      "    policy='step',\n",
      "    warmup='linear',\n",
      "    warmup_iters=500,\n",
      "    warmup_ratio=0.001,\n",
      "    step=[16, 22])\n",
      "runner = dict(type='EpochBasedRunner', max_epochs=24)\n",
      "checkpoint_config = dict(interval=1)\n",
      "log_config = dict(interval=50, hooks=[dict(type='TextLoggerHook')])\n",
      "custom_hooks = [dict(type='NumClassCheckHook')]\n",
      "dist_params = dict(backend='nccl')\n",
      "log_level = 'INFO'\n",
      "load_from = None\n",
      "resume_from = None\n",
      "workflow = [('train', 1)]\n",
      "model = dict(\n",
      "    type='GFL',\n",
      "    pretrained='open-mmlab://res2net101_v1d_26w_4s',\n",
      "    backbone=dict(\n",
      "        type='Res2Net',\n",
      "        depth=101,\n",
      "        num_stages=4,\n",
      "        scales=4,\n",
      "        base_width=26,\n",
      "        out_indices=(0, 1, 2, 3),\n",
      "        frozen_stages=1,\n",
      "        norm_cfg=dict(type='BN', requires_grad=True),\n",
      "        dcn=dict(type='DCN', deform_groups=1, fallback_on_stride=False),\n",
      "        stage_with_dcn=(False, False, True, True),\n",
      "        norm_eval=True,\n",
      "        style='pytorch'),\n",
      "    neck=dict(\n",
      "        type='FPN',\n",
      "        in_channels=[256, 512, 1024, 2048],\n",
      "        out_channels=256,\n",
      "        start_level=1,\n",
      "        add_extra_convs='on_output',\n",
      "        num_outs=5),\n",
      "    bbox_head=dict(\n",
      "        type='GFLHead',\n",
      "        num_classes=11,\n",
      "        in_channels=256,\n",
      "        stacked_convs=4,\n",
      "        feat_channels=256,\n",
      "        anchor_generator=dict(\n",
      "            type='AnchorGenerator',\n",
      "            ratios=[1.0],\n",
      "            octave_base_scale=8,\n",
      "            scales_per_octave=1,\n",
      "            strides=[8, 16, 32, 64, 128]),\n",
      "        loss_cls=dict(\n",
      "            type='QualityFocalLoss',\n",
      "            use_sigmoid=False,\n",
      "            beta=2.0,\n",
      "            loss_weight=1.0),\n",
      "        loss_dfl=dict(type='DistributionFocalLoss', loss_weight=0.25),\n",
      "        reg_max=16,\n",
      "        use_dgqp=True,\n",
      "        reg_topk=4,\n",
      "        reg_channels=64,\n",
      "        add_mean=True,\n",
      "        loss_bbox=dict(type='GIoULoss', loss_weight=2.0)),\n",
      "    train_cfg=None,\n",
      "    test_cfg=dict(\n",
      "        nms_pre=1000,\n",
      "        min_bbox_size=0,\n",
      "        score_thr=0.0,\n",
      "        nms=dict(type='nms', iou_threshold=0.4),\n",
      "        max_per_img=100))\n",
      "seed = 2020\n",
      "gpu_ids = [0]\n",
      "work_dir = './work_dirs/gflv2_r2_101_fpn_dcn_mstrain_2x_aug_trainall'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "classes = (\"UNKNOWN\", \"General trash\", \"Paper\", \"Paper pack\", \"Metal\", \"Glass\", \n",
    "           \"Plastic\", \"Styrofoam\", \"Plastic bag\", \"Battery\", \"Clothing\")\n",
    "# config file 들고오기\n",
    "cfg = Config.fromfile('./configs/gflv2/gflv2_r2_101_fpn_dcn_mstrain_2x_coco.py')\n",
    "\n",
    "PREFIX = '../../input/data/'\n",
    "\n",
    "epoch = 24\n",
    "\n",
    "# dataset 바꾸기\n",
    "\n",
    "# cfg.test_pipeline[1].img_scale = [(1333,600),(1333, 800),(1333,1000)]\n",
    "# cfg.test_pipeline[1].img_scale = [(999, 600),(1333, 800),(1666, 1000)]\n",
    "cfg.test_pipeline[1].img_scale = [(1333,480),(1333,800),(1333,960)]\n",
    "\n",
    "cfg.data.train.classes = classes\n",
    "cfg.data.train.img_prefix = PREFIX\n",
    "cfg.data.train.ann_file = PREFIX + 'train.json'\n",
    "\n",
    "cfg.data.val.classes = classes\n",
    "cfg.data.val.img_prefix = PREFIX\n",
    "cfg.data.val.ann_file = PREFIX + 'val.json'\n",
    "\n",
    "cfg.data.test.classes = classes\n",
    "cfg.data.test.img_prefix = PREFIX\n",
    "cfg.data.test.ann_file = PREFIX + 'test.json'\n",
    "# cfg.data.test.pipeline[1].img_scale = [(1333,600),(1333, 800),(1333,1000)]\n",
    "# cfg.data.test.pipeline[1].img_scale = [(999, 600),(1333, 800),(1666, 1000)]\n",
    "cfg.data.test.pipeline[1].img_scale = [(1333,480),(1333,800),(1333,960)]\n",
    "\n",
    "cfg.model.test_cfg.score_thr=0.00\n",
    "cfg.model.test_cfg.nms=dict(type='nms', iou_threshold=0.4)\n",
    "\n",
    "cfg.data.samples_per_gpu = 8\n",
    "cfg.data.workers_per_gpu = 4\n",
    "\n",
    "cfg.seed=2020\n",
    "cfg.gpu_ids = [0]\n",
    "cfg.work_dir = './work_dirs/gflv2_r2_101_fpn_dcn_mstrain_2x_aug_trainall'\n",
    "\n",
    "cfg.model.bbox_head.num_classes = 11\n",
    "\n",
    "cfg.optimizer_config.grad_clip = dict(max_norm=35, norm_type=2)\n",
    "cfg.model.train_cfg = None\n",
    "\n",
    "# checkpoint path\n",
    "checkpoint_path = os.path.join(cfg.work_dir, f'epoch_{epoch}.pth')\n",
    "\n",
    "print(f'Config:\\n{cfg.pretty_text}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "dataset = build_dataset(cfg.data.test)\n",
    "data_loader = build_dataloader(\n",
    "        dataset,\n",
    "        samples_per_gpu=1,\n",
    "        workers_per_gpu=cfg.data.workers_per_gpu,\n",
    "        dist=False,\n",
    "        shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-19 16:55:20,839 - mmdet - INFO - load model from: open-mmlab://res2net101_v1d_26w_4s\n",
      "2021-05-19 16:55:20,840 - mmdet - INFO - Use load_from_openmmlab loader\n",
      "2021-05-19 16:55:21,066 - mmdet - WARNING - The model and loaded state dict do not match exactly\n",
      "\n",
      "unexpected key in source state_dict: fc.weight, fc.bias\n",
      "\n",
      "missing keys in source state_dict: layer3.0.convs.0.conv_offset.weight, layer3.0.convs.0.conv_offset.bias, layer3.0.convs.1.conv_offset.weight, layer3.0.convs.1.conv_offset.bias, layer3.0.convs.2.conv_offset.weight, layer3.0.convs.2.conv_offset.bias, layer3.1.convs.0.conv_offset.weight, layer3.1.convs.0.conv_offset.bias, layer3.1.convs.1.conv_offset.weight, layer3.1.convs.1.conv_offset.bias, layer3.1.convs.2.conv_offset.weight, layer3.1.convs.2.conv_offset.bias, layer3.2.convs.0.conv_offset.weight, layer3.2.convs.0.conv_offset.bias, layer3.2.convs.1.conv_offset.weight, layer3.2.convs.1.conv_offset.bias, layer3.2.convs.2.conv_offset.weight, layer3.2.convs.2.conv_offset.bias, layer3.3.convs.0.conv_offset.weight, layer3.3.convs.0.conv_offset.bias, layer3.3.convs.1.conv_offset.weight, layer3.3.convs.1.conv_offset.bias, layer3.3.convs.2.conv_offset.weight, layer3.3.convs.2.conv_offset.bias, layer3.4.convs.0.conv_offset.weight, layer3.4.convs.0.conv_offset.bias, layer3.4.convs.1.conv_offset.weight, layer3.4.convs.1.conv_offset.bias, layer3.4.convs.2.conv_offset.weight, layer3.4.convs.2.conv_offset.bias, layer3.5.convs.0.conv_offset.weight, layer3.5.convs.0.conv_offset.bias, layer3.5.convs.1.conv_offset.weight, layer3.5.convs.1.conv_offset.bias, layer3.5.convs.2.conv_offset.weight, layer3.5.convs.2.conv_offset.bias, layer3.6.convs.0.conv_offset.weight, layer3.6.convs.0.conv_offset.bias, layer3.6.convs.1.conv_offset.weight, layer3.6.convs.1.conv_offset.bias, layer3.6.convs.2.conv_offset.weight, layer3.6.convs.2.conv_offset.bias, layer3.7.convs.0.conv_offset.weight, layer3.7.convs.0.conv_offset.bias, layer3.7.convs.1.conv_offset.weight, layer3.7.convs.1.conv_offset.bias, layer3.7.convs.2.conv_offset.weight, layer3.7.convs.2.conv_offset.bias, layer3.8.convs.0.conv_offset.weight, layer3.8.convs.0.conv_offset.bias, layer3.8.convs.1.conv_offset.weight, layer3.8.convs.1.conv_offset.bias, layer3.8.convs.2.conv_offset.weight, layer3.8.convs.2.conv_offset.bias, layer3.9.convs.0.conv_offset.weight, layer3.9.convs.0.conv_offset.bias, layer3.9.convs.1.conv_offset.weight, layer3.9.convs.1.conv_offset.bias, layer3.9.convs.2.conv_offset.weight, layer3.9.convs.2.conv_offset.bias, layer3.10.convs.0.conv_offset.weight, layer3.10.convs.0.conv_offset.bias, layer3.10.convs.1.conv_offset.weight, layer3.10.convs.1.conv_offset.bias, layer3.10.convs.2.conv_offset.weight, layer3.10.convs.2.conv_offset.bias, layer3.11.convs.0.conv_offset.weight, layer3.11.convs.0.conv_offset.bias, layer3.11.convs.1.conv_offset.weight, layer3.11.convs.1.conv_offset.bias, layer3.11.convs.2.conv_offset.weight, layer3.11.convs.2.conv_offset.bias, layer3.12.convs.0.conv_offset.weight, layer3.12.convs.0.conv_offset.bias, layer3.12.convs.1.conv_offset.weight, layer3.12.convs.1.conv_offset.bias, layer3.12.convs.2.conv_offset.weight, layer3.12.convs.2.conv_offset.bias, layer3.13.convs.0.conv_offset.weight, layer3.13.convs.0.conv_offset.bias, layer3.13.convs.1.conv_offset.weight, layer3.13.convs.1.conv_offset.bias, layer3.13.convs.2.conv_offset.weight, layer3.13.convs.2.conv_offset.bias, layer3.14.convs.0.conv_offset.weight, layer3.14.convs.0.conv_offset.bias, layer3.14.convs.1.conv_offset.weight, layer3.14.convs.1.conv_offset.bias, layer3.14.convs.2.conv_offset.weight, layer3.14.convs.2.conv_offset.bias, layer3.15.convs.0.conv_offset.weight, layer3.15.convs.0.conv_offset.bias, layer3.15.convs.1.conv_offset.weight, layer3.15.convs.1.conv_offset.bias, layer3.15.convs.2.conv_offset.weight, layer3.15.convs.2.conv_offset.bias, layer3.16.convs.0.conv_offset.weight, layer3.16.convs.0.conv_offset.bias, layer3.16.convs.1.conv_offset.weight, layer3.16.convs.1.conv_offset.bias, layer3.16.convs.2.conv_offset.weight, layer3.16.convs.2.conv_offset.bias, layer3.17.convs.0.conv_offset.weight, layer3.17.convs.0.conv_offset.bias, layer3.17.convs.1.conv_offset.weight, layer3.17.convs.1.conv_offset.bias, layer3.17.convs.2.conv_offset.weight, layer3.17.convs.2.conv_offset.bias, layer3.18.convs.0.conv_offset.weight, layer3.18.convs.0.conv_offset.bias, layer3.18.convs.1.conv_offset.weight, layer3.18.convs.1.conv_offset.bias, layer3.18.convs.2.conv_offset.weight, layer3.18.convs.2.conv_offset.bias, layer3.19.convs.0.conv_offset.weight, layer3.19.convs.0.conv_offset.bias, layer3.19.convs.1.conv_offset.weight, layer3.19.convs.1.conv_offset.bias, layer3.19.convs.2.conv_offset.weight, layer3.19.convs.2.conv_offset.bias, layer3.20.convs.0.conv_offset.weight, layer3.20.convs.0.conv_offset.bias, layer3.20.convs.1.conv_offset.weight, layer3.20.convs.1.conv_offset.bias, layer3.20.convs.2.conv_offset.weight, layer3.20.convs.2.conv_offset.bias, layer3.21.convs.0.conv_offset.weight, layer3.21.convs.0.conv_offset.bias, layer3.21.convs.1.conv_offset.weight, layer3.21.convs.1.conv_offset.bias, layer3.21.convs.2.conv_offset.weight, layer3.21.convs.2.conv_offset.bias, layer3.22.convs.0.conv_offset.weight, layer3.22.convs.0.conv_offset.bias, layer3.22.convs.1.conv_offset.weight, layer3.22.convs.1.conv_offset.bias, layer3.22.convs.2.conv_offset.weight, layer3.22.convs.2.conv_offset.bias, layer4.0.convs.0.conv_offset.weight, layer4.0.convs.0.conv_offset.bias, layer4.0.convs.1.conv_offset.weight, layer4.0.convs.1.conv_offset.bias, layer4.0.convs.2.conv_offset.weight, layer4.0.convs.2.conv_offset.bias, layer4.1.convs.0.conv_offset.weight, layer4.1.convs.0.conv_offset.bias, layer4.1.convs.1.conv_offset.weight, layer4.1.convs.1.conv_offset.bias, layer4.1.convs.2.conv_offset.weight, layer4.1.convs.2.conv_offset.bias, layer4.2.convs.0.conv_offset.weight, layer4.2.convs.0.conv_offset.bias, layer4.2.convs.1.conv_offset.weight, layer4.2.convs.1.conv_offset.bias, layer4.2.convs.2.conv_offset.weight, layer4.2.convs.2.conv_offset.bias\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use load_from_local loader\n"
     ]
    }
   ],
   "source": [
    "model = build_detector(cfg.model, test_cfg=cfg.get('test_cfg'))\n",
    "checkpoint = load_checkpoint(model, checkpoint_path, map_location='cpu')\n",
    "\n",
    "model.CLASSES = dataset.CLASSES\n",
    "model = MMDataParallel(model.cuda(), device_ids=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 837/837, 3.5 task/s, elapsed: 236s, ETA:     0s"
     ]
    }
   ],
   "source": [
    "output = single_gpu_test(model, data_loader, show_score_thr=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "mmcv.dump(output, 'gflv2.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/ml/code/UniverseNet\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.01s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PredictionString</th>\n",
       "      <th>image_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0 0.07652003 148.49214 294.95898 214.43155 402...</td>\n",
       "      <td>batch_01_vt/0021.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0 0.117910534 286.38577 338.82364 308.37778 37...</td>\n",
       "      <td>batch_01_vt/0028.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0 0.13774404 235.93 461.03674 301.4601 512.0 0...</td>\n",
       "      <td>batch_01_vt/0031.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0 0.091472685 294.83524 158.10371 362.63803 22...</td>\n",
       "      <td>batch_01_vt/0032.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1 0.76338565 388.8329 270.31308 469.49133 344....</td>\n",
       "      <td>batch_01_vt/0070.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    PredictionString              image_id\n",
       "0  0 0.07652003 148.49214 294.95898 214.43155 402...  batch_01_vt/0021.jpg\n",
       "1  0 0.117910534 286.38577 338.82364 308.37778 37...  batch_01_vt/0028.jpg\n",
       "2  0 0.13774404 235.93 461.03674 301.4601 512.0 0...  batch_01_vt/0031.jpg\n",
       "3  0 0.091472685 294.83524 158.10371 362.63803 22...  batch_01_vt/0032.jpg\n",
       "4  1 0.76338565 388.8329 270.31308 469.49133 344....  batch_01_vt/0070.jpg"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_strings = []\n",
    "file_names = []\n",
    "coco = COCO(cfg.data.test.ann_file)\n",
    "imag_ids = coco.getImgIds()\n",
    "\n",
    "class_num = 11\n",
    "for i, out in enumerate(output):\n",
    "    prediction_string = ''\n",
    "    image_info = coco.loadImgs(coco.getImgIds(imgIds=i))[0]\n",
    "    for j in range(class_num):\n",
    "        for o in out[j]:\n",
    "            prediction_string += str(j) + ' ' + str(o[4]) + ' ' + str(o[0]) + ' ' + str(o[1]) + ' ' + str(\n",
    "                o[2]) + ' ' + str(o[3]) + ' '\n",
    "        \n",
    "    prediction_strings.append(prediction_string)\n",
    "    file_names.append(image_info['file_name'])\n",
    "\n",
    "\n",
    "submission = pd.DataFrame()\n",
    "submission['PredictionString'] = prediction_strings\n",
    "submission['image_id'] = file_names\n",
    "submission.to_csv(os.path.join(cfg.work_dir, f'submission_{epoch}.csv'), index=None)\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
