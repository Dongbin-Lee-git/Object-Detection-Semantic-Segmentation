{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import mmcv\n",
    "from mmcv import Config\n",
    "from mmdet.datasets import (build_dataloader, build_dataset,\n",
    "                            replace_ImageToTensor)\n",
    "from mmdet.models import build_detector\n",
    "from mmdet.apis import single_gpu_test\n",
    "from mmcv.runner import load_checkpoint\n",
    "import os\n",
    "from mmcv.parallel import MMDataParallel\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "from pycocotools.coco import COCO\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config:\n",
      "model = dict(\n",
      "    type='CascadeRCNN',\n",
      "    pretrained=None,\n",
      "    backbone=dict(\n",
      "        type='SwinTransformer',\n",
      "        embed_dim=96,\n",
      "        depths=[2, 2, 18, 2],\n",
      "        num_heads=[3, 6, 12, 24],\n",
      "        window_size=7,\n",
      "        mlp_ratio=4.0,\n",
      "        qkv_bias=True,\n",
      "        qk_scale=None,\n",
      "        drop_rate=0.0,\n",
      "        attn_drop_rate=0.0,\n",
      "        drop_path_rate=0.2,\n",
      "        ape=False,\n",
      "        patch_norm=True,\n",
      "        out_indices=(0, 1, 2, 3),\n",
      "        use_checkpoint=False),\n",
      "    neck=dict(\n",
      "        type='FPN',\n",
      "        in_channels=[96, 192, 384, 768],\n",
      "        out_channels=256,\n",
      "        num_outs=5),\n",
      "    rpn_head=dict(\n",
      "        type='RPNHead',\n",
      "        in_channels=256,\n",
      "        feat_channels=256,\n",
      "        anchor_generator=dict(\n",
      "            type='AnchorGenerator',\n",
      "            scales=[8],\n",
      "            ratios=[0.5, 1.0, 2.0],\n",
      "            strides=[4, 8, 16, 32, 64]),\n",
      "        bbox_coder=dict(\n",
      "            type='DeltaXYWHBBoxCoder',\n",
      "            target_means=[0.0, 0.0, 0.0, 0.0],\n",
      "            target_stds=[1.0, 1.0, 1.0, 1.0]),\n",
      "        loss_cls=dict(\n",
      "            type='CrossEntropyLoss', use_sigmoid=True, loss_weight=1.0),\n",
      "        loss_bbox=dict(\n",
      "            type='SmoothL1Loss', beta=0.1111111111111111, loss_weight=1.0)),\n",
      "    roi_head=dict(\n",
      "        type='CascadeRoIHead',\n",
      "        num_stages=3,\n",
      "        stage_loss_weights=[1, 0.5, 0.25],\n",
      "        bbox_roi_extractor=dict(\n",
      "            type='SingleRoIExtractor',\n",
      "            roi_layer=dict(type='RoIAlign', output_size=7, sampling_ratio=0),\n",
      "            out_channels=256,\n",
      "            featmap_strides=[4, 8, 16, 32]),\n",
      "        bbox_head=[\n",
      "            dict(\n",
      "                type='ConvFCBBoxHead',\n",
      "                num_shared_convs=4,\n",
      "                num_shared_fcs=1,\n",
      "                in_channels=256,\n",
      "                conv_out_channels=256,\n",
      "                fc_out_channels=1024,\n",
      "                roi_feat_size=7,\n",
      "                num_classes=80,\n",
      "                bbox_coder=dict(\n",
      "                    type='DeltaXYWHBBoxCoder',\n",
      "                    target_means=[0.0, 0.0, 0.0, 0.0],\n",
      "                    target_stds=[0.1, 0.1, 0.2, 0.2]),\n",
      "                reg_class_agnostic=False,\n",
      "                reg_decoded_bbox=True,\n",
      "                norm_cfg=dict(type='SyncBN', requires_grad=True),\n",
      "                loss_cls=dict(\n",
      "                    type='CrossEntropyLoss',\n",
      "                    use_sigmoid=False,\n",
      "                    loss_weight=1.0),\n",
      "                loss_bbox=dict(type='GIoULoss', loss_weight=10.0)),\n",
      "            dict(\n",
      "                type='ConvFCBBoxHead',\n",
      "                num_shared_convs=4,\n",
      "                num_shared_fcs=1,\n",
      "                in_channels=256,\n",
      "                conv_out_channels=256,\n",
      "                fc_out_channels=1024,\n",
      "                roi_feat_size=7,\n",
      "                num_classes=80,\n",
      "                bbox_coder=dict(\n",
      "                    type='DeltaXYWHBBoxCoder',\n",
      "                    target_means=[0.0, 0.0, 0.0, 0.0],\n",
      "                    target_stds=[0.05, 0.05, 0.1, 0.1]),\n",
      "                reg_class_agnostic=False,\n",
      "                reg_decoded_bbox=True,\n",
      "                norm_cfg=dict(type='SyncBN', requires_grad=True),\n",
      "                loss_cls=dict(\n",
      "                    type='CrossEntropyLoss',\n",
      "                    use_sigmoid=False,\n",
      "                    loss_weight=1.0),\n",
      "                loss_bbox=dict(type='GIoULoss', loss_weight=10.0)),\n",
      "            dict(\n",
      "                type='ConvFCBBoxHead',\n",
      "                num_shared_convs=4,\n",
      "                num_shared_fcs=1,\n",
      "                in_channels=256,\n",
      "                conv_out_channels=256,\n",
      "                fc_out_channels=1024,\n",
      "                roi_feat_size=7,\n",
      "                num_classes=80,\n",
      "                bbox_coder=dict(\n",
      "                    type='DeltaXYWHBBoxCoder',\n",
      "                    target_means=[0.0, 0.0, 0.0, 0.0],\n",
      "                    target_stds=[0.033, 0.033, 0.067, 0.067]),\n",
      "                reg_class_agnostic=False,\n",
      "                reg_decoded_bbox=True,\n",
      "                norm_cfg=dict(type='SyncBN', requires_grad=True),\n",
      "                loss_cls=dict(\n",
      "                    type='CrossEntropyLoss',\n",
      "                    use_sigmoid=False,\n",
      "                    loss_weight=1.0),\n",
      "                loss_bbox=dict(type='GIoULoss', loss_weight=10.0))\n",
      "        ],\n",
      "        mask_roi_extractor=dict(\n",
      "            type='SingleRoIExtractor',\n",
      "            roi_layer=dict(type='RoIAlign', output_size=14, sampling_ratio=0),\n",
      "            out_channels=256,\n",
      "            featmap_strides=[4, 8, 16, 32]),\n",
      "        mask_head=dict(\n",
      "            type='FCNMaskHead',\n",
      "            num_convs=4,\n",
      "            in_channels=256,\n",
      "            conv_out_channels=256,\n",
      "            num_classes=80,\n",
      "            loss_mask=dict(\n",
      "                type='CrossEntropyLoss', use_mask=True, loss_weight=1.0))),\n",
      "    train_cfg=dict(\n",
      "        rpn=dict(\n",
      "            assigner=dict(\n",
      "                type='MaxIoUAssigner',\n",
      "                pos_iou_thr=0.7,\n",
      "                neg_iou_thr=0.3,\n",
      "                min_pos_iou=0.3,\n",
      "                match_low_quality=True,\n",
      "                ignore_iof_thr=-1),\n",
      "            sampler=dict(\n",
      "                type='RandomSampler',\n",
      "                num=256,\n",
      "                pos_fraction=0.5,\n",
      "                neg_pos_ub=-1,\n",
      "                add_gt_as_proposals=False),\n",
      "            allowed_border=0,\n",
      "            pos_weight=-1,\n",
      "            debug=False),\n",
      "        rpn_proposal=dict(\n",
      "            nms_across_levels=False,\n",
      "            nms_pre=2000,\n",
      "            nms_post=2000,\n",
      "            max_per_img=2000,\n",
      "            nms=dict(type='nms', iou_threshold=0.7),\n",
      "            min_bbox_size=0),\n",
      "        rcnn=[\n",
      "            dict(\n",
      "                assigner=dict(\n",
      "                    type='MaxIoUAssigner',\n",
      "                    pos_iou_thr=0.5,\n",
      "                    neg_iou_thr=0.5,\n",
      "                    min_pos_iou=0.5,\n",
      "                    match_low_quality=False,\n",
      "                    ignore_iof_thr=-1),\n",
      "                sampler=dict(\n",
      "                    type='RandomSampler',\n",
      "                    num=512,\n",
      "                    pos_fraction=0.25,\n",
      "                    neg_pos_ub=-1,\n",
      "                    add_gt_as_proposals=True),\n",
      "                mask_size=28,\n",
      "                pos_weight=-1,\n",
      "                debug=False),\n",
      "            dict(\n",
      "                assigner=dict(\n",
      "                    type='MaxIoUAssigner',\n",
      "                    pos_iou_thr=0.6,\n",
      "                    neg_iou_thr=0.6,\n",
      "                    min_pos_iou=0.6,\n",
      "                    match_low_quality=False,\n",
      "                    ignore_iof_thr=-1),\n",
      "                sampler=dict(\n",
      "                    type='RandomSampler',\n",
      "                    num=512,\n",
      "                    pos_fraction=0.25,\n",
      "                    neg_pos_ub=-1,\n",
      "                    add_gt_as_proposals=True),\n",
      "                mask_size=28,\n",
      "                pos_weight=-1,\n",
      "                debug=False),\n",
      "            dict(\n",
      "                assigner=dict(\n",
      "                    type='MaxIoUAssigner',\n",
      "                    pos_iou_thr=0.7,\n",
      "                    neg_iou_thr=0.7,\n",
      "                    min_pos_iou=0.7,\n",
      "                    match_low_quality=False,\n",
      "                    ignore_iof_thr=-1),\n",
      "                sampler=dict(\n",
      "                    type='RandomSampler',\n",
      "                    num=512,\n",
      "                    pos_fraction=0.25,\n",
      "                    neg_pos_ub=-1,\n",
      "                    add_gt_as_proposals=True),\n",
      "                mask_size=28,\n",
      "                pos_weight=-1,\n",
      "                debug=False)\n",
      "        ]),\n",
      "    test_cfg=dict(\n",
      "        rpn=dict(\n",
      "            nms_across_levels=False,\n",
      "            nms_pre=1000,\n",
      "            nms_post=1000,\n",
      "            max_per_img=1000,\n",
      "            nms=dict(type='nms', iou_threshold=0.7),\n",
      "            min_bbox_size=0),\n",
      "        rcnn=dict(\n",
      "            score_thr=0.05,\n",
      "            nms=dict(type='nms', iou_threshold=0.5),\n",
      "            max_per_img=100,\n",
      "            mask_thr_binary=0.5)))\n",
      "dataset_type = 'CocoDataset'\n",
      "data_root = 'data/coco/'\n",
      "img_norm_cfg = dict(\n",
      "    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)\n",
      "train_pipeline = [\n",
      "    dict(type='LoadImageFromFile'),\n",
      "    dict(type='LoadAnnotations', with_bbox=True, with_mask=True),\n",
      "    dict(type='RandomFlip', flip_ratio=0.5),\n",
      "    dict(\n",
      "        type='AutoAugment',\n",
      "        policies=[[{\n",
      "            'type':\n",
      "            'Resize',\n",
      "            'img_scale': [(480, 1333), (512, 1333), (544, 1333), (576, 1333),\n",
      "                          (608, 1333), (640, 1333), (672, 1333), (704, 1333),\n",
      "                          (736, 1333), (768, 1333), (800, 1333)],\n",
      "            'multiscale_mode':\n",
      "            'value',\n",
      "            'keep_ratio':\n",
      "            True\n",
      "        }],\n",
      "                  [{\n",
      "                      'type': 'Resize',\n",
      "                      'img_scale': [(400, 1333), (500, 1333), (600, 1333)],\n",
      "                      'multiscale_mode': 'value',\n",
      "                      'keep_ratio': True\n",
      "                  }, {\n",
      "                      'type': 'RandomCrop',\n",
      "                      'crop_type': 'absolute_range',\n",
      "                      'crop_size': (384, 600),\n",
      "                      'allow_negative_crop': True\n",
      "                  }, {\n",
      "                      'type':\n",
      "                      'Resize',\n",
      "                      'img_scale': [(480, 1333), (512, 1333), (544, 1333),\n",
      "                                    (576, 1333), (608, 1333), (640, 1333),\n",
      "                                    (672, 1333), (704, 1333), (736, 1333),\n",
      "                                    (768, 1333), (800, 1333)],\n",
      "                      'multiscale_mode':\n",
      "                      'value',\n",
      "                      'override':\n",
      "                      True,\n",
      "                      'keep_ratio':\n",
      "                      True\n",
      "                  }]]),\n",
      "    dict(\n",
      "        type='Normalize',\n",
      "        mean=[123.675, 116.28, 103.53],\n",
      "        std=[58.395, 57.12, 57.375],\n",
      "        to_rgb=True),\n",
      "    dict(type='Pad', size_divisor=32),\n",
      "    dict(type='DefaultFormatBundle'),\n",
      "    dict(type='Collect', keys=['img', 'gt_bboxes', 'gt_labels', 'gt_masks'])\n",
      "]\n",
      "test_pipeline = [\n",
      "    dict(type='LoadImageFromFile'),\n",
      "    dict(\n",
      "        type='MultiScaleFlipAug',\n",
      "        img_scale=(1333, 800),\n",
      "        flip=False,\n",
      "        transforms=[\n",
      "            dict(type='Resize', keep_ratio=True),\n",
      "            dict(type='RandomFlip'),\n",
      "            dict(\n",
      "                type='Normalize',\n",
      "                mean=[123.675, 116.28, 103.53],\n",
      "                std=[58.395, 57.12, 57.375],\n",
      "                to_rgb=True),\n",
      "            dict(type='Pad', size_divisor=32),\n",
      "            dict(type='ImageToTensor', keys=['img']),\n",
      "            dict(type='Collect', keys=['img'])\n",
      "        ])\n",
      "]\n",
      "data = dict(\n",
      "    samples_per_gpu=2,\n",
      "    workers_per_gpu=2,\n",
      "    train=dict(\n",
      "        type='CocoDataset',\n",
      "        ann_file='data/coco/annotations/instances_train2017.json',\n",
      "        img_prefix='data/coco/train2017/',\n",
      "        pipeline=[\n",
      "            dict(type='LoadImageFromFile'),\n",
      "            dict(type='LoadAnnotations', with_bbox=True, with_mask=True),\n",
      "            dict(type='RandomFlip', flip_ratio=0.5),\n",
      "            dict(\n",
      "                type='AutoAugment',\n",
      "                policies=[[{\n",
      "                    'type':\n",
      "                    'Resize',\n",
      "                    'img_scale': [(480, 1333), (512, 1333), (544, 1333),\n",
      "                                  (576, 1333), (608, 1333), (640, 1333),\n",
      "                                  (672, 1333), (704, 1333), (736, 1333),\n",
      "                                  (768, 1333), (800, 1333)],\n",
      "                    'multiscale_mode':\n",
      "                    'value',\n",
      "                    'keep_ratio':\n",
      "                    True\n",
      "                }],\n",
      "                          [{\n",
      "                              'type': 'Resize',\n",
      "                              'img_scale': [(400, 1333), (500, 1333),\n",
      "                                            (600, 1333)],\n",
      "                              'multiscale_mode': 'value',\n",
      "                              'keep_ratio': True\n",
      "                          }, {\n",
      "                              'type': 'RandomCrop',\n",
      "                              'crop_type': 'absolute_range',\n",
      "                              'crop_size': (384, 600),\n",
      "                              'allow_negative_crop': True\n",
      "                          }, {\n",
      "                              'type':\n",
      "                              'Resize',\n",
      "                              'img_scale': [(480, 1333), (512, 1333),\n",
      "                                            (544, 1333), (576, 1333),\n",
      "                                            (608, 1333), (640, 1333),\n",
      "                                            (672, 1333), (704, 1333),\n",
      "                                            (736, 1333), (768, 1333),\n",
      "                                            (800, 1333)],\n",
      "                              'multiscale_mode':\n",
      "                              'value',\n",
      "                              'override':\n",
      "                              True,\n",
      "                              'keep_ratio':\n",
      "                              True\n",
      "                          }]]),\n",
      "            dict(\n",
      "                type='Normalize',\n",
      "                mean=[123.675, 116.28, 103.53],\n",
      "                std=[58.395, 57.12, 57.375],\n",
      "                to_rgb=True),\n",
      "            dict(type='Pad', size_divisor=32),\n",
      "            dict(type='DefaultFormatBundle'),\n",
      "            dict(\n",
      "                type='Collect',\n",
      "                keys=['img', 'gt_bboxes', 'gt_labels', 'gt_masks'])\n",
      "        ]),\n",
      "    val=dict(\n",
      "        type='CocoDataset',\n",
      "        ann_file='data/coco/annotations/instances_val2017.json',\n",
      "        img_prefix='data/coco/val2017/',\n",
      "        pipeline=[\n",
      "            dict(type='LoadImageFromFile'),\n",
      "            dict(\n",
      "                type='MultiScaleFlipAug',\n",
      "                img_scale=(1333, 800),\n",
      "                flip=False,\n",
      "                transforms=[\n",
      "                    dict(type='Resize', keep_ratio=True),\n",
      "                    dict(type='RandomFlip'),\n",
      "                    dict(\n",
      "                        type='Normalize',\n",
      "                        mean=[123.675, 116.28, 103.53],\n",
      "                        std=[58.395, 57.12, 57.375],\n",
      "                        to_rgb=True),\n",
      "                    dict(type='Pad', size_divisor=32),\n",
      "                    dict(type='ImageToTensor', keys=['img']),\n",
      "                    dict(type='Collect', keys=['img'])\n",
      "                ])\n",
      "        ]),\n",
      "    test=dict(\n",
      "        type='CocoDataset',\n",
      "        ann_file='data/coco/annotations/instances_val2017.json',\n",
      "        img_prefix='data/coco/val2017/',\n",
      "        pipeline=[\n",
      "            dict(type='LoadImageFromFile'),\n",
      "            dict(\n",
      "                type='MultiScaleFlipAug',\n",
      "                img_scale=(1333, 800),\n",
      "                flip=False,\n",
      "                transforms=[\n",
      "                    dict(type='Resize', keep_ratio=True),\n",
      "                    dict(type='RandomFlip'),\n",
      "                    dict(\n",
      "                        type='Normalize',\n",
      "                        mean=[123.675, 116.28, 103.53],\n",
      "                        std=[58.395, 57.12, 57.375],\n",
      "                        to_rgb=True),\n",
      "                    dict(type='Pad', size_divisor=32),\n",
      "                    dict(type='ImageToTensor', keys=['img']),\n",
      "                    dict(type='Collect', keys=['img'])\n",
      "                ])\n",
      "        ]))\n",
      "evaluation = dict(metric=['bbox', 'segm'])\n",
      "optimizer = dict(\n",
      "    type='AdamW',\n",
      "    lr=0.0001,\n",
      "    betas=(0.9, 0.999),\n",
      "    weight_decay=0.05,\n",
      "    paramwise_cfg=dict(\n",
      "        custom_keys=dict(\n",
      "            absolute_pos_embed=dict(decay_mult=0.0),\n",
      "            relative_position_bias_table=dict(decay_mult=0.0),\n",
      "            norm=dict(decay_mult=0.0))))\n",
      "optimizer_config = dict(\n",
      "    grad_clip=None,\n",
      "    type='DistOptimizerHook',\n",
      "    update_interval=1,\n",
      "    coalesce=True,\n",
      "    bucket_size_mb=-1,\n",
      "    use_fp16=True)\n",
      "lr_config = dict(\n",
      "    policy='step',\n",
      "    warmup='linear',\n",
      "    warmup_iters=500,\n",
      "    warmup_ratio=0.001,\n",
      "    step=[27, 33])\n",
      "runner = dict(type='EpochBasedRunnerAmp', max_epochs=36)\n",
      "checkpoint_config = dict(interval=1)\n",
      "log_config = dict(interval=50, hooks=[dict(type='TextLoggerHook')])\n",
      "custom_hooks = [dict(type='NumClassCheckHook')]\n",
      "dist_params = dict(backend='nccl')\n",
      "log_level = 'INFO'\n",
      "load_from = None\n",
      "resume_from = None\n",
      "workflow = [('train', 1)]\n",
      "fp16 = None\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cfg = Config.fromfile('/opt/ml/Swin-Transformer-Object-Detection/configs/swin/cascade_mask_rcnn_swin_small_patch4_window7_mstrain_480-800_giou_4conv1f_adamw_3x_coco.py')\n",
    "print(f'Config:\\n{cfg.pretty_text}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config:\n",
      "model = dict(\n",
      "    type='FasterRCNN',\n",
      "    pretrained='torchvision://resnet50',\n",
      "    backbone=dict(\n",
      "        type='ResNet',\n",
      "        depth=50,\n",
      "        num_stages=4,\n",
      "        out_indices=(0, 1, 2, 3),\n",
      "        frozen_stages=1,\n",
      "        norm_cfg=dict(type='BN', requires_grad=True),\n",
      "        norm_eval=True,\n",
      "        style='pytorch'),\n",
      "    neck=dict(\n",
      "        type='FPN',\n",
      "        in_channels=[256, 512, 1024, 2048],\n",
      "        out_channels=256,\n",
      "        num_outs=5),\n",
      "    rpn_head=dict(\n",
      "        type='RPNHead',\n",
      "        in_channels=256,\n",
      "        feat_channels=256,\n",
      "        anchor_generator=dict(\n",
      "            type='AnchorGenerator',\n",
      "            scales=[8],\n",
      "            ratios=[0.5, 1.0, 2.0],\n",
      "            strides=[4, 8, 16, 32, 64]),\n",
      "        bbox_coder=dict(\n",
      "            type='DeltaXYWHBBoxCoder',\n",
      "            target_means=[0.0, 0.0, 0.0, 0.0],\n",
      "            target_stds=[1.0, 1.0, 1.0, 1.0]),\n",
      "        loss_cls=dict(\n",
      "            type='CrossEntropyLoss', use_sigmoid=True, loss_weight=1.0),\n",
      "        loss_bbox=dict(type='L1Loss', loss_weight=1.0)),\n",
      "    roi_head=dict(\n",
      "        type='StandardRoIHead',\n",
      "        bbox_roi_extractor=dict(\n",
      "            type='SingleRoIExtractor',\n",
      "            roi_layer=dict(type='RoIAlign', output_size=7, sampling_ratio=0),\n",
      "            out_channels=256,\n",
      "            featmap_strides=[4, 8, 16, 32]),\n",
      "        bbox_head=dict(\n",
      "            type='Shared2FCBBoxHead',\n",
      "            in_channels=256,\n",
      "            fc_out_channels=1024,\n",
      "            roi_feat_size=7,\n",
      "            num_classes=11,\n",
      "            bbox_coder=dict(\n",
      "                type='DeltaXYWHBBoxCoder',\n",
      "                target_means=[0.0, 0.0, 0.0, 0.0],\n",
      "                target_stds=[0.1, 0.1, 0.2, 0.2]),\n",
      "            reg_class_agnostic=False,\n",
      "            loss_cls=dict(\n",
      "                type='CrossEntropyLoss', use_sigmoid=False, loss_weight=1.0),\n",
      "            loss_bbox=dict(type='L1Loss', loss_weight=1.0))),\n",
      "    train_cfg=None,\n",
      "    test_cfg=dict(\n",
      "        rpn=dict(\n",
      "            nms_pre=1000,\n",
      "            max_per_img=1000,\n",
      "            nms=dict(type='nms', iou_threshold=0.7),\n",
      "            min_bbox_size=0),\n",
      "        rcnn=dict(\n",
      "            score_thr=0.05,\n",
      "            nms=dict(type='nms', iou_threshold=0.5),\n",
      "            max_per_img=100)))\n",
      "dataset_type = 'CocoDataset'\n",
      "data_root = 'data/coco/'\n",
      "img_norm_cfg = dict(\n",
      "    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)\n",
      "train_pipeline = [\n",
      "    dict(type='LoadImageFromFile'),\n",
      "    dict(type='LoadAnnotations', with_bbox=True),\n",
      "    dict(type='Resize', img_scale=(1333, 800), keep_ratio=True),\n",
      "    dict(type='RandomFlip', flip_ratio=0.5),\n",
      "    dict(\n",
      "        type='Normalize',\n",
      "        mean=[123.675, 116.28, 103.53],\n",
      "        std=[58.395, 57.12, 57.375],\n",
      "        to_rgb=True),\n",
      "    dict(type='Pad', size_divisor=32),\n",
      "    dict(type='DefaultFormatBundle'),\n",
      "    dict(type='Collect', keys=['img', 'gt_bboxes', 'gt_labels'])\n",
      "]\n",
      "test_pipeline = [\n",
      "    dict(type='LoadImageFromFile'),\n",
      "    dict(\n",
      "        type='MultiScaleFlipAug',\n",
      "        img_scale=(1333, 800),\n",
      "        flip=False,\n",
      "        transforms=[\n",
      "            dict(type='Resize', keep_ratio=True),\n",
      "            dict(type='RandomFlip'),\n",
      "            dict(\n",
      "                type='Normalize',\n",
      "                mean=[123.675, 116.28, 103.53],\n",
      "                std=[58.395, 57.12, 57.375],\n",
      "                to_rgb=True),\n",
      "            dict(type='Pad', size_divisor=32),\n",
      "            dict(type='ImageToTensor', keys=['img']),\n",
      "            dict(type='Collect', keys=['img'])\n",
      "        ])\n",
      "]\n",
      "data = dict(\n",
      "    samples_per_gpu=32,\n",
      "    workers_per_gpu=4,\n",
      "    train=dict(\n",
      "        type='CocoDataset',\n",
      "        ann_file='../../input/data/train.json',\n",
      "        img_prefix='../../input/data/',\n",
      "        pipeline=[\n",
      "            dict(type='LoadImageFromFile'),\n",
      "            dict(type='LoadAnnotations', with_bbox=True),\n",
      "            dict(type='Resize', img_scale=(512, 512), keep_ratio=True),\n",
      "            dict(type='RandomFlip', flip_ratio=0.5),\n",
      "            dict(\n",
      "                type='Normalize',\n",
      "                mean=[123.675, 116.28, 103.53],\n",
      "                std=[58.395, 57.12, 57.375],\n",
      "                to_rgb=True),\n",
      "            dict(type='Pad', size_divisor=32),\n",
      "            dict(type='DefaultFormatBundle'),\n",
      "            dict(type='Collect', keys=['img', 'gt_bboxes', 'gt_labels'])\n",
      "        ],\n",
      "        classes=('UNKNOWN', 'General trash', 'Paper', 'Paper pack', 'Metal',\n",
      "                 'Glass', 'Plastic', 'Styrofoam', 'Plastic bag', 'Battery',\n",
      "                 'Clothing')),\n",
      "    val=dict(\n",
      "        type='CocoDataset',\n",
      "        ann_file='../../input/data/val.json',\n",
      "        img_prefix='../../input/data/',\n",
      "        pipeline=[\n",
      "            dict(type='LoadImageFromFile'),\n",
      "            dict(\n",
      "                type='MultiScaleFlipAug',\n",
      "                img_scale=(1024, 1024),\n",
      "                flip=False,\n",
      "                transforms=[\n",
      "                    dict(type='Resize', keep_ratio=True),\n",
      "                    dict(type='RandomFlip'),\n",
      "                    dict(\n",
      "                        type='Normalize',\n",
      "                        mean=[123.675, 116.28, 103.53],\n",
      "                        std=[58.395, 57.12, 57.375],\n",
      "                        to_rgb=True),\n",
      "                    dict(type='Pad', size_divisor=32),\n",
      "                    dict(type='ImageToTensor', keys=['img']),\n",
      "                    dict(type='Collect', keys=['img'])\n",
      "                ])\n",
      "        ],\n",
      "        classes=('UNKNOWN', 'General trash', 'Paper', 'Paper pack', 'Metal',\n",
      "                 'Glass', 'Plastic', 'Styrofoam', 'Plastic bag', 'Battery',\n",
      "                 'Clothing')),\n",
      "    test=dict(\n",
      "        type='CocoDataset',\n",
      "        ann_file='../../input/data/test.json',\n",
      "        img_prefix='../../input/data/',\n",
      "        pipeline=[\n",
      "            dict(type='LoadImageFromFile'),\n",
      "            dict(\n",
      "                type='MultiScaleFlipAug',\n",
      "                img_scale=[(1000, 600), (1333, 800), (1666, 1000)],\n",
      "                flip=False,\n",
      "                transforms=[\n",
      "                    dict(type='Resize', keep_ratio=True),\n",
      "                    dict(type='RandomFlip'),\n",
      "                    dict(\n",
      "                        type='Normalize',\n",
      "                        mean=[123.675, 116.28, 103.53],\n",
      "                        std=[58.395, 57.12, 57.375],\n",
      "                        to_rgb=True),\n",
      "                    dict(type='Pad', size_divisor=32),\n",
      "                    dict(type='ImageToTensor', keys=['img']),\n",
      "                    dict(type='Collect', keys=['img'])\n",
      "                ])\n",
      "        ],\n",
      "        classes=('UNKNOWN', 'General trash', 'Paper', 'Paper pack', 'Metal',\n",
      "                 'Glass', 'Plastic', 'Styrofoam', 'Plastic bag', 'Battery',\n",
      "                 'Clothing')))\n",
      "evaluation = dict(interval=1, metric='bbox')\n",
      "optimizer = dict(type='SGD', lr=0.02, momentum=0.9, weight_decay=0.0001)\n",
      "optimizer_config = dict(grad_clip=dict(max_norm=35, norm_type=2))\n",
      "lr_config = dict(\n",
      "    policy='step',\n",
      "    warmup='linear',\n",
      "    warmup_iters=500,\n",
      "    warmup_ratio=0.001,\n",
      "    step=[16, 22])\n",
      "runner = dict(type='EpochBasedRunner', max_epochs=24)\n",
      "checkpoint_config = dict(interval=1)\n",
      "log_config = dict(interval=50, hooks=[dict(type='TextLoggerHook')])\n",
      "custom_hooks = [dict(type='NumClassCheckHook')]\n",
      "dist_params = dict(backend='nccl')\n",
      "log_level = 'INFO'\n",
      "load_from = None\n",
      "resume_from = None\n",
      "workflow = [('train', 1)]\n",
      "seed = 2020\n",
      "gpu_ids = [0]\n",
      "work_dir = './work_dirs/faster_rcnn_2x_batch8_wontrans_s50c65_num3'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "classes = (\"UNKNOWN\", \"General trash\", \"Paper\", \"Paper pack\", \"Metal\", \"Glass\", \n",
    "           \"Plastic\", \"Styrofoam\", \"Plastic bag\", \"Battery\", \"Clothing\")\n",
    "# config file 들고오기\n",
    "# cfg = Config.fromfile('./configs/faster_rcnn/faster_rcnn_r50_fpn_1x_coco.py')\n",
    "cfg = Config.fromfile('./configs/faster_rcnn/faster_rcnn_r50_fpn_2x_coco.py')\n",
    "\n",
    "PREFIX = '../../input/data/'\n",
    "\n",
    "\n",
    "# dataset 바꾸기\n",
    "cfg.data.train.classes = classes\n",
    "cfg.data.train.img_prefix = PREFIX\n",
    "cfg.data.train.ann_file = PREFIX + 'train.json'\n",
    "cfg.data.train.pipeline[2]['img_scale'] = (512, 512)\n",
    "\n",
    "cfg.data.val.classes = classes\n",
    "cfg.data.val.img_prefix = PREFIX\n",
    "cfg.data.val.ann_file = PREFIX + 'val.json'\n",
    "cfg.data.val.pipeline[1]['img_scale'] = (1024, 1024)\n",
    "\n",
    "cfg.data.test.classes = classes\n",
    "cfg.data.test.img_prefix = PREFIX\n",
    "cfg.data.test.ann_file = PREFIX + 'test.json'\n",
    "cfg.data.test.pipeline[1]['img_scale'] = (1333, 800)\n",
    "\n",
    "cfg.data.samples_per_gpu = 32\n",
    "cfg.data.workers_per_gpu = 4\n",
    "\n",
    "cfg.seed=2020\n",
    "cfg.gpu_ids = [0]\n",
    "cfg.work_dir = './work_dirs/faster_rcnn_2x_batch8_wontrans_s50c65_num3'\n",
    "cfg.model.roi_head.bbox_head.num_classes = 11\n",
    "\n",
    "cfg.optimizer_config.grad_clip = dict(max_norm=35, norm_type=2)\n",
    "cfg.model.train_cfg = None\n",
    "\n",
    "\n",
    "\n",
    "# Multi scale\n",
    "cfg.data.test.pipeline[1]['img_scale'] = [(1000, 600),(1333, 800),(1666, 1000)]\n",
    "# cfg.data.test.pipeline[1]['img_scale'] = (512, 512)\n",
    "\n",
    "# checkpoint path\n",
    "epoch = 24\n",
    "# checkpoint_path = os.path.join(cfg.work_dir, f'epoch_{epoch}.pth')\n",
    "checkpoint_path = os.path.join(cfg.work_dir, f'epoch_{epoch}.pth')\n",
    "\n",
    "print(f'Config:\\n{cfg.pretty_text}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config:\n",
      "dataset_type = 'CocoDataset'\n",
      "data_root = 'data/coco/'\n",
      "img_norm_cfg = dict(\n",
      "    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)\n",
      "train_pipeline = [\n",
      "    dict(type='LoadImageFromFile'),\n",
      "    dict(\n",
      "        type='InstaBoost',\n",
      "        action_candidate=('normal', 'horizontal', 'skip'),\n",
      "        action_prob=(1, 0, 0),\n",
      "        scale=(0.8, 1.2),\n",
      "        dx=15,\n",
      "        dy=15,\n",
      "        theta=(-1, 1),\n",
      "        color_prob=0.0,\n",
      "        hflag=False,\n",
      "        aug_ratio=0.5),\n",
      "    dict(\n",
      "        type='LoadAnnotations', with_bbox=True, with_mask=True, with_seg=True),\n",
      "    dict(type='RandomFlip', flip_ratio=0.5),\n",
      "    dict(\n",
      "        type='AutoAugment',\n",
      "        policies=[[{\n",
      "            'type':\n",
      "            'Resize',\n",
      "            'img_scale': [(512, 512), (576, 576), (640, 640), (704, 704),\n",
      "                          (768, 768), (832, 832), (896, 896), (960, 960),\n",
      "                          (1024, 1024)],\n",
      "            'multiscale_mode':\n",
      "            'value',\n",
      "            'keep_ratio':\n",
      "            True\n",
      "        }],\n",
      "                  [{\n",
      "                      'type': 'Resize',\n",
      "                      'img_scale': [(512, 512), (768, 768), (1024, 1024)],\n",
      "                      'multiscale_mode': 'value',\n",
      "                      'keep_ratio': True\n",
      "                  }, {\n",
      "                      'type': 'RandomCrop',\n",
      "                      'crop_type': 'absolute_range',\n",
      "                      'crop_size': (512, 512),\n",
      "                      'allow_negative_crop': True\n",
      "                  }, {\n",
      "                      'type':\n",
      "                      'Resize',\n",
      "                      'img_scale': [(512, 512), (576, 576), (640, 640),\n",
      "                                    (704, 704), (768, 768), (832, 832),\n",
      "                                    (896, 896), (960, 960), (1024, 1024)],\n",
      "                      'multiscale_mode':\n",
      "                      'value',\n",
      "                      'override':\n",
      "                      True,\n",
      "                      'keep_ratio':\n",
      "                      True\n",
      "                  }]]),\n",
      "    dict(type='Pad', size_divisor=32),\n",
      "    dict(\n",
      "        type='Albu',\n",
      "        transforms=[\n",
      "            dict(\n",
      "                type='Cutout',\n",
      "                num_holes=30,\n",
      "                max_h_size=30,\n",
      "                max_w_size=30,\n",
      "                fill_value=[103.53, 116.28, 123.675],\n",
      "                p=0.1),\n",
      "            dict(\n",
      "                type='RandomBrightnessContrast',\n",
      "                brightness_limit=[0.1, 0.3],\n",
      "                contrast_limit=[0.1, 0.3],\n",
      "                p=0.2),\n",
      "            dict(\n",
      "                type='OneOf',\n",
      "                transforms=[\n",
      "                    dict(\n",
      "                        type='RGBShift',\n",
      "                        r_shift_limit=10,\n",
      "                        g_shift_limit=10,\n",
      "                        b_shift_limit=10,\n",
      "                        p=1.0),\n",
      "                    dict(\n",
      "                        type='HueSaturationValue',\n",
      "                        hue_shift_limit=20,\n",
      "                        sat_shift_limit=30,\n",
      "                        val_shift_limit=20,\n",
      "                        p=1.0),\n",
      "                    dict(type='RandomGamma'),\n",
      "                    dict(type='CLAHE')\n",
      "                ],\n",
      "                p=0.1),\n",
      "            dict(\n",
      "                type='JpegCompression',\n",
      "                quality_lower=85,\n",
      "                quality_upper=95,\n",
      "                p=0.2),\n",
      "            dict(type='ChannelShuffle', p=0.1),\n",
      "            dict(\n",
      "                type='OneOf',\n",
      "                transforms=[\n",
      "                    dict(type='Blur', blur_limit=3, p=1.0),\n",
      "                    dict(type='MedianBlur', blur_limit=3, p=1.0),\n",
      "                    dict(type='MotionBlur'),\n",
      "                    dict(type='GaussNoise'),\n",
      "                    dict(type='ImageCompression', quality_lower=75)\n",
      "                ],\n",
      "                p=0.1),\n",
      "            dict(type='RandomRotate90', p=0.5)\n",
      "        ],\n",
      "        bbox_params=dict(\n",
      "            type='BboxParams',\n",
      "            format='pascal_voc',\n",
      "            label_fields=['gt_labels'],\n",
      "            min_visibility=0.0,\n",
      "            filter_lost_elements=True),\n",
      "        keymap=dict(img='image', gt_masks='masks', gt_bboxes='bboxes'),\n",
      "        update_pad_shape=False,\n",
      "        skip_img_without_anno=True),\n",
      "    dict(\n",
      "        type='Normalize',\n",
      "        mean=[123.675, 116.28, 103.53],\n",
      "        std=[58.395, 57.12, 57.375],\n",
      "        to_rgb=True),\n",
      "    dict(type='SegRescale', scale_factor=0.125),\n",
      "    dict(type='DefaultFormatBundle'),\n",
      "    dict(\n",
      "        type='Collect',\n",
      "        keys=['img', 'gt_bboxes', 'gt_labels', 'gt_masks', 'gt_semantic_seg'])\n",
      "]\n",
      "test_pipeline = [\n",
      "    dict(type='LoadImageFromFile'),\n",
      "    dict(\n",
      "        type='MultiScaleFlipAug',\n",
      "        img_scale=(1333, 800),\n",
      "        flip=False,\n",
      "        transforms=[\n",
      "            dict(type='Resize', keep_ratio=True),\n",
      "            dict(type='RandomFlip'),\n",
      "            dict(\n",
      "                type='Normalize',\n",
      "                mean=[123.675, 116.28, 103.53],\n",
      "                std=[58.395, 57.12, 57.375],\n",
      "                to_rgb=True),\n",
      "            dict(type='Pad', size_divisor=32),\n",
      "            dict(type='ImageToTensor', keys=['img']),\n",
      "            dict(type='Collect', keys=['img'])\n",
      "        ])\n",
      "]\n",
      "data = dict(\n",
      "    samples_per_gpu=2,\n",
      "    workers_per_gpu=4,\n",
      "    train=dict(\n",
      "        type='CocoDataset',\n",
      "        ann_file='/opt/ml/input/data/train_all.json',\n",
      "        img_prefix='/opt/ml/input/data/',\n",
      "        pipeline=[\n",
      "            dict(type='LoadImageFromFile'),\n",
      "            dict(\n",
      "                type='InstaBoost',\n",
      "                action_candidate=('normal', 'horizontal', 'skip'),\n",
      "                action_prob=(1, 0, 0),\n",
      "                scale=(0.8, 1.2),\n",
      "                dx=15,\n",
      "                dy=15,\n",
      "                theta=(-1, 1),\n",
      "                color_prob=0.0,\n",
      "                hflag=False,\n",
      "                aug_ratio=0.5),\n",
      "            dict(\n",
      "                type='LoadAnnotations',\n",
      "                with_bbox=True,\n",
      "                with_mask=True,\n",
      "                with_seg=True),\n",
      "            dict(type='RandomFlip', flip_ratio=0.5),\n",
      "            dict(\n",
      "                type='AutoAugment',\n",
      "                policies=[[{\n",
      "                    'type':\n",
      "                    'Resize',\n",
      "                    'img_scale':\n",
      "                    [(512, 512), (576, 576), (640, 640), (704, 704),\n",
      "                     (768, 768), (832, 832), (896, 896), (960, 960),\n",
      "                     (1024, 1024)],\n",
      "                    'multiscale_mode':\n",
      "                    'value',\n",
      "                    'keep_ratio':\n",
      "                    True\n",
      "                }],\n",
      "                          [{\n",
      "                              'type': 'Resize',\n",
      "                              'img_scale': [(512, 512), (768, 768),\n",
      "                                            (1024, 1024)],\n",
      "                              'multiscale_mode': 'value',\n",
      "                              'keep_ratio': True\n",
      "                          }, {\n",
      "                              'type': 'RandomCrop',\n",
      "                              'crop_type': 'absolute_range',\n",
      "                              'crop_size': (512, 512),\n",
      "                              'allow_negative_crop': True\n",
      "                          }, {\n",
      "                              'type':\n",
      "                              'Resize',\n",
      "                              'img_scale': [(512, 512), (576, 576), (640, 640),\n",
      "                                            (704, 704), (768, 768), (832, 832),\n",
      "                                            (896, 896), (960, 960),\n",
      "                                            (1024, 1024)],\n",
      "                              'multiscale_mode':\n",
      "                              'value',\n",
      "                              'override':\n",
      "                              True,\n",
      "                              'keep_ratio':\n",
      "                              True\n",
      "                          }]]),\n",
      "            dict(type='Pad', size_divisor=32),\n",
      "            dict(\n",
      "                type='Albu',\n",
      "                transforms=[\n",
      "                    dict(\n",
      "                        type='Cutout',\n",
      "                        num_holes=30,\n",
      "                        max_h_size=30,\n",
      "                        max_w_size=30,\n",
      "                        fill_value=[103.53, 116.28, 123.675],\n",
      "                        p=0.1),\n",
      "                    dict(\n",
      "                        type='RandomBrightnessContrast',\n",
      "                        brightness_limit=[0.1, 0.3],\n",
      "                        contrast_limit=[0.1, 0.3],\n",
      "                        p=0.2),\n",
      "                    dict(\n",
      "                        type='OneOf',\n",
      "                        transforms=[\n",
      "                            dict(\n",
      "                                type='RGBShift',\n",
      "                                r_shift_limit=10,\n",
      "                                g_shift_limit=10,\n",
      "                                b_shift_limit=10,\n",
      "                                p=1.0),\n",
      "                            dict(\n",
      "                                type='HueSaturationValue',\n",
      "                                hue_shift_limit=20,\n",
      "                                sat_shift_limit=30,\n",
      "                                val_shift_limit=20,\n",
      "                                p=1.0),\n",
      "                            dict(type='RandomGamma'),\n",
      "                            dict(type='CLAHE')\n",
      "                        ],\n",
      "                        p=0.1),\n",
      "                    dict(\n",
      "                        type='JpegCompression',\n",
      "                        quality_lower=85,\n",
      "                        quality_upper=95,\n",
      "                        p=0.2),\n",
      "                    dict(type='ChannelShuffle', p=0.1),\n",
      "                    dict(\n",
      "                        type='OneOf',\n",
      "                        transforms=[\n",
      "                            dict(type='Blur', blur_limit=3, p=1.0),\n",
      "                            dict(type='MedianBlur', blur_limit=3, p=1.0),\n",
      "                            dict(type='MotionBlur'),\n",
      "                            dict(type='GaussNoise'),\n",
      "                            dict(type='ImageCompression', quality_lower=75)\n",
      "                        ],\n",
      "                        p=0.1),\n",
      "                    dict(type='RandomRotate90', p=0.5)\n",
      "                ],\n",
      "                bbox_params=dict(\n",
      "                    type='BboxParams',\n",
      "                    format='pascal_voc',\n",
      "                    label_fields=['gt_labels'],\n",
      "                    min_visibility=0.0,\n",
      "                    filter_lost_elements=True),\n",
      "                keymap=dict(img='image', gt_masks='masks', gt_bboxes='bboxes'),\n",
      "                update_pad_shape=False,\n",
      "                skip_img_without_anno=True),\n",
      "            dict(\n",
      "                type='Normalize',\n",
      "                mean=[123.675, 116.28, 103.53],\n",
      "                std=[58.395, 57.12, 57.375],\n",
      "                to_rgb=True),\n",
      "            dict(type='SegRescale', scale_factor=0.125),\n",
      "            dict(type='DefaultFormatBundle'),\n",
      "            dict(\n",
      "                type='Collect',\n",
      "                keys=[\n",
      "                    'img', 'gt_bboxes', 'gt_labels', 'gt_masks',\n",
      "                    'gt_semantic_seg'\n",
      "                ])\n",
      "        ],\n",
      "        classes=('UNKNOWN', 'General trash', 'Paper', 'Paper pack', 'Metal',\n",
      "                 'Glass', 'Plastic', 'Styrofoam', 'Plastic bag', 'Battery',\n",
      "                 'Clothing'),\n",
      "        seg_prefix='/opt/ml/input/data/'),\n",
      "    val=dict(\n",
      "        type='CocoDataset',\n",
      "        ann_file='/opt/ml/input/data/val.json',\n",
      "        img_prefix='/opt/ml/input/data/',\n",
      "        pipeline=[\n",
      "            dict(type='LoadImageFromFile'),\n",
      "            dict(\n",
      "                type='MultiScaleFlipAug',\n",
      "                img_scale=(1333, 800),\n",
      "                flip=False,\n",
      "                transforms=[\n",
      "                    dict(type='Resize', keep_ratio=True),\n",
      "                    dict(type='RandomFlip'),\n",
      "                    dict(\n",
      "                        type='Normalize',\n",
      "                        mean=[123.675, 116.28, 103.53],\n",
      "                        std=[58.395, 57.12, 57.375],\n",
      "                        to_rgb=True),\n",
      "                    dict(type='Pad', size_divisor=32),\n",
      "                    dict(type='ImageToTensor', keys=['img']),\n",
      "                    dict(type='Collect', keys=['img'])\n",
      "                ])\n",
      "        ],\n",
      "        classes=('UNKNOWN', 'General trash', 'Paper', 'Paper pack', 'Metal',\n",
      "                 'Glass', 'Plastic', 'Styrofoam', 'Plastic bag', 'Battery',\n",
      "                 'Clothing')),\n",
      "    test=dict(\n",
      "        type='CocoDataset',\n",
      "        ann_file='/opt/ml/input/data/test.json',\n",
      "        img_prefix='/opt/ml/input/data/',\n",
      "        pipeline=[\n",
      "            dict(type='LoadImageFromFile'),\n",
      "            dict(\n",
      "                type='MultiScaleFlipAug',\n",
      "                img_scale=[(320, 320), (480, 480), (640, 640), (800, 800),\n",
      "                           (960, 960), (1120, 1120)],\n",
      "                flip=False,\n",
      "                transforms=[\n",
      "                    dict(type='Resize', keep_ratio=True),\n",
      "                    dict(type='RandomFlip'),\n",
      "                    dict(\n",
      "                        type='Normalize',\n",
      "                        mean=[123.675, 116.28, 103.53],\n",
      "                        std=[58.395, 57.12, 57.375],\n",
      "                        to_rgb=True),\n",
      "                    dict(type='Pad', size_divisor=32),\n",
      "                    dict(type='ImageToTensor', keys=['img']),\n",
      "                    dict(type='Collect', keys=['img'])\n",
      "                ])\n",
      "        ],\n",
      "        classes=('UNKNOWN', 'General trash', 'Paper', 'Paper pack', 'Metal',\n",
      "                 'Glass', 'Plastic', 'Styrofoam', 'Plastic bag', 'Battery',\n",
      "                 'Clothing')))\n",
      "evaluation = dict(metric=['bbox', 'segm'])\n",
      "optimizer = dict(\n",
      "    type='AdamW',\n",
      "    lr=0.0001,\n",
      "    betas=(0.9, 0.999),\n",
      "    weight_decay=0.05,\n",
      "    paramwise_cfg=dict(\n",
      "        custom_keys=dict(\n",
      "            absolute_pos_embed=dict(decay_mult=0.0),\n",
      "            relative_position_bias_table=dict(decay_mult=0.0),\n",
      "            norm=dict(decay_mult=0.0))))\n",
      "optimizer_config = dict(\n",
      "    grad_clip=dict(max_norm=35, norm_type=2),\n",
      "    type='DistOptimizerHook',\n",
      "    update_interval=1,\n",
      "    coalesce=True,\n",
      "    bucket_size_mb=-1,\n",
      "    use_fp16=True)\n",
      "lr_config = dict(\n",
      "    policy='CosineAnnealing',\n",
      "    warmup='linear',\n",
      "    warmup_iters=3000,\n",
      "    warmup_ratio=0.0001,\n",
      "    min_lr_ratio=1e-07)\n",
      "runner = dict(type='EpochBasedRunnerAmp', max_epochs=60)\n",
      "checkpoint_config = dict(max_keep_ckpts=2, interval=1)\n",
      "log_config = dict(interval=50, hooks=[dict(type='TextLoggerHook')])\n",
      "custom_hooks = [dict(type='NumClassCheckHook')]\n",
      "dist_params = dict(backend='nccl')\n",
      "log_level = 'INFO'\n",
      "load_from = 'cascade_mask_rcnn_swin_small_patch4_window7.pth'\n",
      "resume_from = None\n",
      "workflow = [('train', 1)]\n",
      "model = dict(\n",
      "    type='HybridTaskCascade',\n",
      "    pretrained=None,\n",
      "    backbone=dict(\n",
      "        type='SwinTransformer',\n",
      "        embed_dim=96,\n",
      "        depths=[2, 2, 18, 2],\n",
      "        num_heads=[3, 6, 12, 24],\n",
      "        window_size=7,\n",
      "        mlp_ratio=4.0,\n",
      "        qkv_bias=True,\n",
      "        qk_scale=None,\n",
      "        drop_rate=0.0,\n",
      "        attn_drop_rate=0.0,\n",
      "        drop_path_rate=0.2,\n",
      "        ape=False,\n",
      "        patch_norm=True,\n",
      "        out_indices=(0, 1, 2, 3),\n",
      "        use_checkpoint=False),\n",
      "    neck=dict(\n",
      "        type='FPN',\n",
      "        in_channels=[96, 192, 384, 768],\n",
      "        out_channels=256,\n",
      "        num_outs=5),\n",
      "    rpn_head=dict(\n",
      "        type='RPNHead',\n",
      "        in_channels=256,\n",
      "        feat_channels=256,\n",
      "        anchor_generator=dict(\n",
      "            type='AnchorGenerator',\n",
      "            scales=[8],\n",
      "            ratios=[0.5, 1.0, 2.0],\n",
      "            strides=[4, 8, 16, 32, 64]),\n",
      "        bbox_coder=dict(\n",
      "            type='DeltaXYWHBBoxCoder',\n",
      "            target_means=[0.0, 0.0, 0.0, 0.0],\n",
      "            target_stds=[1.0, 1.0, 1.0, 1.0]),\n",
      "        loss_cls=dict(\n",
      "            type='CrossEntropyLoss', use_sigmoid=True, loss_weight=1.0),\n",
      "        loss_bbox=dict(\n",
      "            type='SmoothL1Loss', beta=0.1111111111111111, loss_weight=1.0)),\n",
      "    roi_head=dict(\n",
      "        type='HybridTaskCascadeRoIHead',\n",
      "        semantic_roi_extractor=dict(\n",
      "            type='SingleRoIExtractor',\n",
      "            roi_layer=dict(type='RoIAlign', output_size=14, sampling_ratio=0),\n",
      "            out_channels=256,\n",
      "            featmap_strides=[8]),\n",
      "        semantic_head=dict(\n",
      "            type='FusedSemanticHead',\n",
      "            num_ins=5,\n",
      "            fusion_level=1,\n",
      "            num_convs=4,\n",
      "            in_channels=256,\n",
      "            conv_out_channels=256,\n",
      "            num_classes=24,\n",
      "            ignore_label=255,\n",
      "            loss_weight=0.2),\n",
      "        interleaved=True,\n",
      "        mask_info_flow=True,\n",
      "        num_stages=3,\n",
      "        stage_loss_weights=[1, 0.5, 0.25],\n",
      "        bbox_roi_extractor=dict(\n",
      "            type='SingleRoIExtractor',\n",
      "            roi_layer=dict(type='RoIAlign', output_size=7, sampling_ratio=0),\n",
      "            out_channels=256,\n",
      "            featmap_strides=[4, 8, 16, 32]),\n",
      "        bbox_head=[\n",
      "            dict(\n",
      "                type='Shared2FCBBoxHead',\n",
      "                in_channels=256,\n",
      "                fc_out_channels=1024,\n",
      "                roi_feat_size=7,\n",
      "                num_classes=11,\n",
      "                bbox_coder=dict(\n",
      "                    type='DeltaXYWHBBoxCoder',\n",
      "                    target_means=[0.0, 0.0, 0.0, 0.0],\n",
      "                    target_stds=[0.1, 0.1, 0.2, 0.2]),\n",
      "                reg_class_agnostic=True,\n",
      "                loss_cls=dict(\n",
      "                    type='CrossEntropyLoss',\n",
      "                    use_sigmoid=False,\n",
      "                    loss_weight=1.0),\n",
      "                loss_bbox=dict(type='SmoothL1Loss', beta=1.0,\n",
      "                               loss_weight=1.0)),\n",
      "            dict(\n",
      "                type='Shared2FCBBoxHead',\n",
      "                in_channels=256,\n",
      "                fc_out_channels=1024,\n",
      "                roi_feat_size=7,\n",
      "                num_classes=11,\n",
      "                bbox_coder=dict(\n",
      "                    type='DeltaXYWHBBoxCoder',\n",
      "                    target_means=[0.0, 0.0, 0.0, 0.0],\n",
      "                    target_stds=[0.05, 0.05, 0.1, 0.1]),\n",
      "                reg_class_agnostic=True,\n",
      "                loss_cls=dict(\n",
      "                    type='CrossEntropyLoss',\n",
      "                    use_sigmoid=False,\n",
      "                    loss_weight=1.0),\n",
      "                loss_bbox=dict(type='SmoothL1Loss', beta=1.0,\n",
      "                               loss_weight=1.0)),\n",
      "            dict(\n",
      "                type='Shared2FCBBoxHead',\n",
      "                in_channels=256,\n",
      "                fc_out_channels=1024,\n",
      "                roi_feat_size=7,\n",
      "                num_classes=11,\n",
      "                bbox_coder=dict(\n",
      "                    type='DeltaXYWHBBoxCoder',\n",
      "                    target_means=[0.0, 0.0, 0.0, 0.0],\n",
      "                    target_stds=[0.033, 0.033, 0.067, 0.067]),\n",
      "                reg_class_agnostic=True,\n",
      "                loss_cls=dict(\n",
      "                    type='CrossEntropyLoss',\n",
      "                    use_sigmoid=False,\n",
      "                    loss_weight=1.0),\n",
      "                loss_bbox=dict(type='SmoothL1Loss', beta=1.0, loss_weight=1.0))\n",
      "        ],\n",
      "        mask_roi_extractor=dict(\n",
      "            type='SingleRoIExtractor',\n",
      "            roi_layer=dict(type='RoIAlign', output_size=14, sampling_ratio=0),\n",
      "            out_channels=256,\n",
      "            featmap_strides=[4, 8, 16, 32]),\n",
      "        mask_head=[\n",
      "            dict(\n",
      "                type='HTCMaskHead',\n",
      "                with_conv_res=False,\n",
      "                num_convs=4,\n",
      "                in_channels=256,\n",
      "                conv_out_channels=256,\n",
      "                num_classes=11,\n",
      "                loss_mask=dict(\n",
      "                    type='CrossEntropyLoss', use_mask=True, loss_weight=1.0)),\n",
      "            dict(\n",
      "                type='HTCMaskHead',\n",
      "                num_convs=4,\n",
      "                in_channels=256,\n",
      "                conv_out_channels=256,\n",
      "                num_classes=11,\n",
      "                loss_mask=dict(\n",
      "                    type='CrossEntropyLoss', use_mask=True, loss_weight=1.0)),\n",
      "            dict(\n",
      "                type='HTCMaskHead',\n",
      "                num_convs=4,\n",
      "                in_channels=256,\n",
      "                conv_out_channels=256,\n",
      "                num_classes=11,\n",
      "                loss_mask=dict(\n",
      "                    type='CrossEntropyLoss', use_mask=True, loss_weight=1.0))\n",
      "        ]),\n",
      "    train_cfg=None,\n",
      "    test_cfg=dict(\n",
      "        rpn=dict(\n",
      "            nms_pre=1000,\n",
      "            max_per_img=1000,\n",
      "            nms=dict(type='nms', iou_threshold=0.7),\n",
      "            min_bbox_size=0),\n",
      "        rcnn=dict(\n",
      "            score_thr=0.0,\n",
      "            nms=dict(type='nms', iou_threshold=0.4),\n",
      "            max_per_img=100,\n",
      "            mask_thr_binary=0.5)))\n",
      "albu_train_transforms = [\n",
      "    dict(\n",
      "        type='Cutout',\n",
      "        num_holes=30,\n",
      "        max_h_size=30,\n",
      "        max_w_size=30,\n",
      "        fill_value=[103.53, 116.28, 123.675],\n",
      "        p=0.1),\n",
      "    dict(\n",
      "        type='RandomBrightnessContrast',\n",
      "        brightness_limit=[0.1, 0.3],\n",
      "        contrast_limit=[0.1, 0.3],\n",
      "        p=0.2),\n",
      "    dict(\n",
      "        type='OneOf',\n",
      "        transforms=[\n",
      "            dict(\n",
      "                type='RGBShift',\n",
      "                r_shift_limit=10,\n",
      "                g_shift_limit=10,\n",
      "                b_shift_limit=10,\n",
      "                p=1.0),\n",
      "            dict(\n",
      "                type='HueSaturationValue',\n",
      "                hue_shift_limit=20,\n",
      "                sat_shift_limit=30,\n",
      "                val_shift_limit=20,\n",
      "                p=1.0),\n",
      "            dict(type='RandomGamma'),\n",
      "            dict(type='CLAHE')\n",
      "        ],\n",
      "        p=0.1),\n",
      "    dict(type='JpegCompression', quality_lower=85, quality_upper=95, p=0.2),\n",
      "    dict(type='ChannelShuffle', p=0.1),\n",
      "    dict(\n",
      "        type='OneOf',\n",
      "        transforms=[\n",
      "            dict(type='Blur', blur_limit=3, p=1.0),\n",
      "            dict(type='MedianBlur', blur_limit=3, p=1.0),\n",
      "            dict(type='MotionBlur'),\n",
      "            dict(type='GaussNoise'),\n",
      "            dict(type='ImageCompression', quality_lower=75)\n",
      "        ],\n",
      "        p=0.1),\n",
      "    dict(type='RandomRotate90', p=0.5)\n",
      "]\n",
      "fp16 = None\n",
      "work_dir = './work_dirs/swin2_1team'\n",
      "seed = 2020\n",
      "gpu_ids = [0]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "classes = (\"UNKNOWN\", \"General trash\", \"Paper\", \"Paper pack\", \"Metal\", \"Glass\", \n",
    "           \"Plastic\", \"Styrofoam\", \"Plastic bag\", \"Battery\", \"Clothing\")\n",
    "# config file 들고오기\n",
    "# cfg = Config.fromfile('/opt/ml/Swin-Transformer-Object-Detection/configs/swin/siwn_one.py')\n",
    "cfg = Config.fromfile('/opt/ml/Swin-Transformer-Object-Detection/configs/swin/new_our_swin.py')\n",
    "# cfg = Config.fromfile('/opt/ml/Swin-Transformer-Object-Detection/configs/swin/noapex.py')\n",
    "\n",
    "cfg.work_dir = './work_dirs/swin2_1team'\n",
    "\n",
    "# PREFIX = '/opt/ml/input/data/'\n",
    "\n",
    "cfg.data.train.classes = classes\n",
    "# cfg.data.train.img_prefix = PREFIX\n",
    "# cfg.data.train.ann_file = PREFIX + 'train_all.json'\n",
    "\n",
    "PREFIX = '/opt/ml/input/data/'\n",
    "\n",
    "cfg.data.val.classes = classes\n",
    "cfg.data.val.img_prefix = PREFIX\n",
    "cfg.data.val.ann_file = PREFIX + 'val.json'\n",
    "\n",
    "cfg.data.test.classes = classes\n",
    "cfg.data.test.img_prefix = PREFIX\n",
    "cfg.data.test.ann_file = PREFIX + 'test.json'\n",
    "\n",
    "cfg.data.samples_per_gpu = 2\n",
    "cfg.data.workers_per_gpu = 4\n",
    "\n",
    "cfg.seed=2020\n",
    "# cfg.gpu_ids = [0]\n",
    "\n",
    "cfg.lr_config = dict(policy='CosineAnnealing',warmup='linear',warmup_iters=3000,\n",
    "                    warmup_ratio=0.0001, min_lr_ratio=1e-7)\n",
    "\n",
    "cfg.load_from = \"cascade_mask_rcnn_swin_small_patch4_window7.pth\" # 얘는 pretrain 모델 가져오는 경로\n",
    "\n",
    "cfg.gpu_ids = [0]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# dataset 바꾸기\n",
    "cfg.data.train.classes = classes\n",
    "cfg.data.train.img_prefix = PREFIX\n",
    "cfg.data.train.ann_file = PREFIX + 'train_all.json'\n",
    "cfg.data.train.seg_prefix=PREFIX                        ### 요놈 ###\n",
    "\n",
    "\n",
    "cfg.data.train=dict(\n",
    "        type='CocoDataset',\n",
    "        ann_file='/opt/ml/input/data/train_all.json',\n",
    "        img_prefix='/opt/ml/input/data/',\n",
    "        pipeline=[\n",
    "            dict(type='LoadImageFromFile'),\n",
    "            dict(\n",
    "                type='InstaBoost',\n",
    "                action_candidate=('normal', 'horizontal', 'skip'),\n",
    "                action_prob=(1, 0, 0),\n",
    "                scale=(0.8, 1.2),\n",
    "                dx=15,\n",
    "                dy=15,\n",
    "                theta=(-1, 1),\n",
    "                color_prob=0.0,\n",
    "                hflag=False,\n",
    "                aug_ratio=0.5),\n",
    "            dict(\n",
    "                type='LoadAnnotations',\n",
    "                with_bbox=True,\n",
    "                with_mask=True,\n",
    "                with_seg=True),\n",
    "            dict(type='RandomFlip', flip_ratio=0.5),\n",
    "            dict(\n",
    "                type='AutoAugment',\n",
    "                policies=[[{\n",
    "                    'type':\n",
    "                    'Resize',\n",
    "                    'img_scale':\n",
    "                    [(512, 512), (576, 576), (640, 640), (704, 704),\n",
    "                     (768, 768), (832, 832), (896, 896), (960, 960),\n",
    "                     (1024, 1024)],\n",
    "                    'multiscale_mode':\n",
    "                    'value',\n",
    "                    'keep_ratio':\n",
    "                    True\n",
    "                }],\n",
    "                          [{\n",
    "                              'type': 'Resize',\n",
    "                              'img_scale': [(512, 512), (768, 768),\n",
    "                                            (1024, 1024)],\n",
    "                              'multiscale_mode': 'value',\n",
    "                              'keep_ratio': True\n",
    "                          }, {\n",
    "                              'type': 'RandomCrop',\n",
    "                              'crop_type': 'absolute_range',\n",
    "                              'crop_size': (512, 512),\n",
    "                              'allow_negative_crop': True\n",
    "                          }, {\n",
    "                              'type':\n",
    "                              'Resize',\n",
    "                              'img_scale': [(512, 512), (576, 576), (640, 640),\n",
    "                                            (704, 704), (768, 768), (832, 832),\n",
    "                                            (896, 896), (960, 960),\n",
    "                                            (1024, 1024)],\n",
    "                              'multiscale_mode':\n",
    "                              'value',\n",
    "                              'override':\n",
    "                              True,\n",
    "                              'keep_ratio':\n",
    "                              True\n",
    "                          }]]),\n",
    "            dict(type='Pad', size_divisor=32),\n",
    "            dict(\n",
    "                type='Albu',\n",
    "                transforms=[\n",
    "                    dict(\n",
    "                        type='Cutout',\n",
    "                        num_holes=30,\n",
    "                        max_h_size=30,\n",
    "                        max_w_size=30,\n",
    "                        fill_value=[103.53, 116.28, 123.675],\n",
    "                        p=0.1),\n",
    "                    dict(\n",
    "                        type='RandomBrightnessContrast',\n",
    "                        brightness_limit=[0.1, 0.3],\n",
    "                        contrast_limit=[0.1, 0.3],\n",
    "                        p=0.2),\n",
    "                    dict(\n",
    "                        type='OneOf',\n",
    "                        transforms=[\n",
    "                            dict(\n",
    "                                type='RGBShift',\n",
    "                                r_shift_limit=10,\n",
    "                                g_shift_limit=10,\n",
    "                                b_shift_limit=10,\n",
    "                                p=1.0),\n",
    "                            dict(\n",
    "                                type='HueSaturationValue',\n",
    "                                hue_shift_limit=20,\n",
    "                                sat_shift_limit=30,\n",
    "                                val_shift_limit=20,\n",
    "                                p=1.0),\n",
    "                            dict(type='RandomGamma'),\n",
    "                            dict(type='CLAHE')\n",
    "                        ],\n",
    "                        p=0.1),\n",
    "                    dict(\n",
    "                        type='JpegCompression',\n",
    "                        quality_lower=85,\n",
    "                        quality_upper=95,\n",
    "                        p=0.2),\n",
    "                    dict(type='ChannelShuffle', p=0.1),\n",
    "                    dict(\n",
    "                        type='OneOf',\n",
    "                        transforms=[\n",
    "                            dict(type='Blur', blur_limit=3, p=1.0),\n",
    "                            dict(type='MedianBlur', blur_limit=3, p=1.0),\n",
    "                            dict(type='MotionBlur'),\n",
    "                            dict(type='GaussNoise'),\n",
    "                            dict(type='ImageCompression', quality_lower=75)\n",
    "                        ],\n",
    "                        p=0.1),\n",
    "                    dict(type='RandomRotate90', p=0.5)\n",
    "                ],\n",
    "                bbox_params=dict(\n",
    "                    type='BboxParams',\n",
    "                    format='pascal_voc',\n",
    "                    label_fields=['gt_labels'],\n",
    "                    min_visibility=0.0,\n",
    "                    filter_lost_elements=True),\n",
    "                keymap=dict(img='image', gt_masks='masks', gt_bboxes='bboxes'),\n",
    "                update_pad_shape=False,\n",
    "                skip_img_without_anno=True),\n",
    "            dict(\n",
    "                type='Normalize',\n",
    "                mean=[123.675, 116.28, 103.53],\n",
    "                std=[58.395, 57.12, 57.375],\n",
    "                to_rgb=True),\n",
    "            dict(type='SegRescale', scale_factor=0.125),\n",
    "            dict(type='DefaultFormatBundle'),\n",
    "            dict(\n",
    "                type='Collect',\n",
    "                keys=[\n",
    "                    'img', 'gt_bboxes', 'gt_labels', 'gt_masks',\n",
    "                    'gt_semantic_seg'\n",
    "                ])\n",
    "        ],\n",
    "        classes=('UNKNOWN', 'General trash', 'Paper', 'Paper pack', 'Metal',\n",
    "                 'Glass', 'Plastic', 'Styrofoam', 'Plastic bag', 'Battery',\n",
    "                 'Clothing'),\n",
    "        seg_prefix='/opt/ml/input/data/')\n",
    "\n",
    "cfg.checkpoint_config = dict(max_keep_ckpts=2, interval=1)\n",
    "\n",
    "cfg.optimizer_config.grad_clip = dict(max_norm=35, norm_type=2)\n",
    "cfg.model.train_cfg = None\n",
    "\n",
    "# Multi scale\n",
    "# cfg.data.test.pipeline[1]['img_scale'] = [(1000, 600),(1333, 800),(1666, 1000)]\n",
    "cfg.data.test.pipeline[1]['img_scale'] = [(320, 320), (480, 480), (640, 640), (800,800), (960,960), (1120,1120)]\n",
    "\n",
    "\n",
    "# cfg.data.test.pipeline[1]['img_scale'] = (512, 512)\n",
    "\n",
    "# checkpoint path\n",
    "epoch = 29\n",
    "# checkpoint_path = os.path.join(cfg.work_dir, f'epoch_{epoch}.pth')\n",
    "checkpoint_path = os.path.join(cfg.work_dir, f'epoch_{epoch}.pth')\n",
    "\n",
    "\n",
    "# cfg.model.test_cfg=dict(\n",
    "#         rpn=dict(\n",
    "#             nms_pre=1000,\n",
    "#             max_per_img=1000,\n",
    "#             nms=dict(type='nms', iou_threshold=0.7),\n",
    "#             min_bbox_size=0),\n",
    "#         rcnn=dict(\n",
    "#             score_thr=0.001,\n",
    "#             nms=dict(type='nms', iou_threshold=0.4),\n",
    "#             max_per_img=100,\n",
    "#             mask_thr_binary=0.5))\n",
    "\n",
    "# cfg.model.test_cfg=dict(\n",
    "#         rpn=dict(\n",
    "#             nms_pre=1000,\n",
    "#             max_per_img=1000,\n",
    "#             nms=dict(type='nms', iou_threshold=0.7),\n",
    "#             min_bbox_size=0),\n",
    "#         rcnn=dict(\n",
    "#             score_thr=0.000,\n",
    "#             nms=dict(type='nms', iou_threshold=0.4),\n",
    "#             max_per_img=100,\n",
    "#             mask_thr_binary=0.5))\n",
    "\n",
    "\n",
    "print(f'Config:\\n{cfg.pretty_text}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "dataset = build_dataset(cfg.data.test)\n",
    "data_loader = build_dataloader(\n",
    "        dataset,\n",
    "        samples_per_gpu=1,\n",
    "        workers_per_gpu=cfg.data.workers_per_gpu,\n",
    "        dist=False,\n",
    "        shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use load_from_local loader\n"
     ]
    }
   ],
   "source": [
    "model = build_detector(cfg.model, test_cfg=cfg.get('test_cfg'))\n",
    "checkpoint = load_checkpoint(model, checkpoint_path, map_location='cpu')\n",
    "\n",
    "model.CLASSES = dataset.CLASSES\n",
    "model = MMDataParallel(model.cuda(), device_ids=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 837/837, 0.7 task/s, elapsed: 1180s, ETA:     0s"
     ]
    }
   ],
   "source": [
    "output = single_gpu_test(model, data_loader, show_score_thr=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "mmcv.dump(output, \"./work_dirs/swin2_1team/epoch_29.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(output[3][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.01s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PredictionString</th>\n",
       "      <th>image_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0 0.004770118 162.08495 73.9816 254.09549 163....</td>\n",
       "      <td>batch_01_vt/0021.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0 0.005677956 0.05226949 222.2557 106.20748 35...</td>\n",
       "      <td>batch_01_vt/0028.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0 0.015770484 233.219 460.03217 303.2897 511.0...</td>\n",
       "      <td>batch_01_vt/0031.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0 0.00587218 298.22028 159.11554 361.67505 225...</td>\n",
       "      <td>batch_01_vt/0032.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1 0.42366537 388.9406 265.778 488.71725 343.90...</td>\n",
       "      <td>batch_01_vt/0070.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    PredictionString              image_id\n",
       "0  0 0.004770118 162.08495 73.9816 254.09549 163....  batch_01_vt/0021.jpg\n",
       "1  0 0.005677956 0.05226949 222.2557 106.20748 35...  batch_01_vt/0028.jpg\n",
       "2  0 0.015770484 233.219 460.03217 303.2897 511.0...  batch_01_vt/0031.jpg\n",
       "3  0 0.00587218 298.22028 159.11554 361.67505 225...  batch_01_vt/0032.jpg\n",
       "4  1 0.42366537 388.9406 265.778 488.71725 343.90...  batch_01_vt/0070.jpg"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_strings = []\n",
    "file_names = []\n",
    "coco = COCO(cfg.data.test.ann_file)\n",
    "imag_ids = coco.getImgIds()\n",
    "\n",
    "class_num = 11\n",
    "for i, out in enumerate(output):\n",
    "    out = out[0]\n",
    "    prediction_string = ''\n",
    "    image_info = coco.loadImgs(coco.getImgIds(imgIds=i))[0]\n",
    "    for j in range(class_num):\n",
    "        for o in out[j]:\n",
    "            prediction_string += str(j) + ' ' + str(o[4]) + ' ' + str(o[0]) + ' ' + str(o[1]) + ' ' + str(o[2]) + ' ' + str(o[3]) + ' '\n",
    "        \n",
    "    prediction_strings.append(prediction_string)\n",
    "    file_names.append(image_info['file_name'])\n",
    "\n",
    "\n",
    "submission = pd.DataFrame()\n",
    "submission['PredictionString'] = prediction_strings\n",
    "submission['image_id'] = file_names\n",
    "submission.to_csv(os.path.join(cfg.work_dir, f'submission_{epoch}_swin_29.csv'), index=None)\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-16 01:51:10,175 - mmdet - INFO - load model from: torchvision://resnet50\n",
      "2021-05-16 01:51:10,176 - mmdet - INFO - Use load_from_torchvision loader\n",
      "2021-05-16 01:51:10,592 - mmdet - WARNING - The model and loaded state dict do not match exactly\n",
      "\n",
      "unexpected key in source state_dict: fc.weight, fc.bias\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use load_from_local loader\n",
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 837/837, 8.4 task/s, elapsed: 99s, ETA:     0sloading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PredictionString</th>\n",
       "      <th>image_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1 0.27244276 156.50906 65.47758 256.4415 166.9...</td>\n",
       "      <td>batch_01_vt/0021.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1 0.11728452 102.37038 0.0 196.40446 256.51465...</td>\n",
       "      <td>batch_01_vt/0028.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0 0.083333336 106.56012 0.0 336.13547 0.0 0 0....</td>\n",
       "      <td>batch_01_vt/0031.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2 0.68128747 296.69302 156.01796 362.49228 233...</td>\n",
       "      <td>batch_01_vt/0032.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1 0.26740214 367.3728 166.96545 506.89734 336....</td>\n",
       "      <td>batch_01_vt/0070.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    PredictionString              image_id\n",
       "0  1 0.27244276 156.50906 65.47758 256.4415 166.9...  batch_01_vt/0021.jpg\n",
       "1  1 0.11728452 102.37038 0.0 196.40446 256.51465...  batch_01_vt/0028.jpg\n",
       "2  0 0.083333336 106.56012 0.0 336.13547 0.0 0 0....  batch_01_vt/0031.jpg\n",
       "3  2 0.68128747 296.69302 156.01796 362.49228 233...  batch_01_vt/0032.jpg\n",
       "4  1 0.26740214 367.3728 166.96545 506.89734 336....  batch_01_vt/0070.jpg"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg.work_dir = './work_dirs/faster_rcnn_kfold0'\n",
    "model = build_detector(cfg.model, test_cfg=cfg.get('test_cfg'))\n",
    "checkpoint_path = os.path.join(cfg.work_dir, f'epoch_{epoch}.pth')\n",
    "checkpoint = load_checkpoint(model, checkpoint_path, map_location='cpu')\n",
    "\n",
    "model.CLASSES = dataset.CLASSES\n",
    "model = MMDataParallel(model.cuda(), device_ids=[0])\n",
    "output = single_gpu_test(model, data_loader, show_score_thr=0.05)\n",
    "prediction_strings = []\n",
    "file_names = []\n",
    "coco = COCO(cfg.data.test.ann_file)\n",
    "imag_ids = coco.getImgIds()\n",
    "\n",
    "class_num = 11\n",
    "for i, out in enumerate(output):\n",
    "    prediction_string = ''\n",
    "    image_info = coco.loadImgs(coco.getImgIds(imgIds=i))[0]\n",
    "    for j in range(class_num):\n",
    "        for o in out[j]:\n",
    "            prediction_string += str(j) + ' ' + str(o[4]) + ' ' + str(o[0]) + ' ' + str(o[1]) + ' ' + str(\n",
    "                o[2]) + ' ' + str(o[3]) + ' '\n",
    "        \n",
    "    prediction_strings.append(prediction_string)\n",
    "    file_names.append(image_info['file_name'])\n",
    "\n",
    "\n",
    "submission = pd.DataFrame()\n",
    "submission['PredictionString'] = prediction_strings\n",
    "submission['image_id'] = file_names\n",
    "submission.to_csv(os.path.join(cfg.work_dir, f'submission_{epoch}_1x_fold1_.csv'), index=None)\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-16 01:52:51,476 - mmdet - INFO - load model from: torchvision://resnet50\n",
      "2021-05-16 01:52:51,477 - mmdet - INFO - Use load_from_torchvision loader\n",
      "2021-05-16 01:52:51,729 - mmdet - WARNING - The model and loaded state dict do not match exactly\n",
      "\n",
      "unexpected key in source state_dict: fc.weight, fc.bias\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use load_from_local loader\n",
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 837/837, 8.9 task/s, elapsed: 94s, ETA:     0sloading annotations into memory...\n",
      "Done (t=0.01s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PredictionString</th>\n",
       "      <th>image_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1 0.06447051 157.11705 55.205334 255.71165 167...</td>\n",
       "      <td>batch_01_vt/0021.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1 0.12336707 0.0 216.91666 294.43307 463.7288 ...</td>\n",
       "      <td>batch_01_vt/0028.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0 0.083333336 462.4012 0.0 494.1273 0.0 0 0.08...</td>\n",
       "      <td>batch_01_vt/0031.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1 0.061352003 336.65472 184.47388 364.66217 23...</td>\n",
       "      <td>batch_01_vt/0032.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1 0.16919202 356.94113 161.57474 510.03732 329...</td>\n",
       "      <td>batch_01_vt/0070.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    PredictionString              image_id\n",
       "0  1 0.06447051 157.11705 55.205334 255.71165 167...  batch_01_vt/0021.jpg\n",
       "1  1 0.12336707 0.0 216.91666 294.43307 463.7288 ...  batch_01_vt/0028.jpg\n",
       "2  0 0.083333336 462.4012 0.0 494.1273 0.0 0 0.08...  batch_01_vt/0031.jpg\n",
       "3  1 0.061352003 336.65472 184.47388 364.66217 23...  batch_01_vt/0032.jpg\n",
       "4  1 0.16919202 356.94113 161.57474 510.03732 329...  batch_01_vt/0070.jpg"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg.work_dir = './work_dirs/faster_rcnn_kfold1'\n",
    "model = build_detector(cfg.model, test_cfg=cfg.get('test_cfg'))\n",
    "checkpoint_path = os.path.join(cfg.work_dir, f'epoch_{epoch}.pth')\n",
    "checkpoint = load_checkpoint(model, checkpoint_path, map_location='cpu')\n",
    "\n",
    "model.CLASSES = dataset.CLASSES\n",
    "model = MMDataParallel(model.cuda(), device_ids=[0])\n",
    "output = single_gpu_test(model, data_loader, show_score_thr=0.05)\n",
    "prediction_strings = []\n",
    "file_names = []\n",
    "coco = COCO(cfg.data.test.ann_file)\n",
    "imag_ids = coco.getImgIds()\n",
    "\n",
    "class_num = 11\n",
    "for i, out in enumerate(output):\n",
    "    prediction_string = ''\n",
    "    image_info = coco.loadImgs(coco.getImgIds(imgIds=i))[0]\n",
    "    for j in range(class_num):\n",
    "        for o in out[j]:\n",
    "            prediction_string += str(j) + ' ' + str(o[4]) + ' ' + str(o[0]) + ' ' + str(o[1]) + ' ' + str(\n",
    "                o[2]) + ' ' + str(o[3]) + ' '\n",
    "        \n",
    "    prediction_strings.append(prediction_string)\n",
    "    file_names.append(image_info['file_name'])\n",
    "\n",
    "\n",
    "submission = pd.DataFrame()\n",
    "submission['PredictionString'] = prediction_strings\n",
    "submission['image_id'] = file_names\n",
    "submission.to_csv(os.path.join(cfg.work_dir, f'submission_{epoch}_1x_fold1_.csv'), index=None)\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-16 01:54:29,108 - mmdet - INFO - load model from: torchvision://resnet50\n",
      "2021-05-16 01:54:29,109 - mmdet - INFO - Use load_from_torchvision loader\n",
      "2021-05-16 01:54:29,328 - mmdet - WARNING - The model and loaded state dict do not match exactly\n",
      "\n",
      "unexpected key in source state_dict: fc.weight, fc.bias\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use load_from_local loader\n",
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 837/837, 9.3 task/s, elapsed: 90s, ETA:     0sloading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PredictionString</th>\n",
       "      <th>image_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1 0.2573298 142.61311 49.239693 256.6261 172.7...</td>\n",
       "      <td>batch_01_vt/0021.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1 0.20577663 100.95451 0.0 193.90013 251.7756 ...</td>\n",
       "      <td>batch_01_vt/0028.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0 0.083333336 297.2498 0.0 505.07144 0.0 0 0.0...</td>\n",
       "      <td>batch_01_vt/0031.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1 0.082291365 297.06442 161.45549 364.91428 22...</td>\n",
       "      <td>batch_01_vt/0032.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1 0.18720336 153.96759 298.29453 293.4585 407....</td>\n",
       "      <td>batch_01_vt/0070.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    PredictionString              image_id\n",
       "0  1 0.2573298 142.61311 49.239693 256.6261 172.7...  batch_01_vt/0021.jpg\n",
       "1  1 0.20577663 100.95451 0.0 193.90013 251.7756 ...  batch_01_vt/0028.jpg\n",
       "2  0 0.083333336 297.2498 0.0 505.07144 0.0 0 0.0...  batch_01_vt/0031.jpg\n",
       "3  1 0.082291365 297.06442 161.45549 364.91428 22...  batch_01_vt/0032.jpg\n",
       "4  1 0.18720336 153.96759 298.29453 293.4585 407....  batch_01_vt/0070.jpg"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg.work_dir = './work_dirs/faster_rcnn_kfold2'\n",
    "model = build_detector(cfg.model, test_cfg=cfg.get('test_cfg'))\n",
    "checkpoint_path = os.path.join(cfg.work_dir, f'epoch_{epoch}.pth')\n",
    "checkpoint = load_checkpoint(model, checkpoint_path, map_location='cpu')\n",
    "\n",
    "model.CLASSES = dataset.CLASSES\n",
    "model = MMDataParallel(model.cuda(), device_ids=[0])\n",
    "output = single_gpu_test(model, data_loader, show_score_thr=0.05)\n",
    "prediction_strings = []\n",
    "file_names = []\n",
    "coco = COCO(cfg.data.test.ann_file)\n",
    "imag_ids = coco.getImgIds()\n",
    "\n",
    "class_num = 11\n",
    "for i, out in enumerate(output):\n",
    "    prediction_string = ''\n",
    "    image_info = coco.loadImgs(coco.getImgIds(imgIds=i))[0]\n",
    "    for j in range(class_num):\n",
    "        for o in out[j]:\n",
    "            prediction_string += str(j) + ' ' + str(o[4]) + ' ' + str(o[0]) + ' ' + str(o[1]) + ' ' + str(\n",
    "                o[2]) + ' ' + str(o[3]) + ' '\n",
    "        \n",
    "    prediction_strings.append(prediction_string)\n",
    "    file_names.append(image_info['file_name'])\n",
    "\n",
    "\n",
    "submission = pd.DataFrame()\n",
    "submission['PredictionString'] = prediction_strings\n",
    "submission['image_id'] = file_names\n",
    "submission.to_csv(os.path.join(cfg.work_dir, f'submission_{epoch}_faster_rcnn_2x_batch8_wontrans_pseudo1_s30c55.csv'), index=None)\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-16 01:56:02,418 - mmdet - INFO - load model from: torchvision://resnet50\n",
      "2021-05-16 01:56:02,419 - mmdet - INFO - Use load_from_torchvision loader\n",
      "2021-05-16 01:56:02,652 - mmdet - WARNING - The model and loaded state dict do not match exactly\n",
      "\n",
      "unexpected key in source state_dict: fc.weight, fc.bias\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use load_from_local loader\n",
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 837/837, 10.2 task/s, elapsed: 82s, ETA:     0sloading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PredictionString</th>\n",
       "      <th>image_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1 0.36776102 343.9898 35.461994 364.5382 46.19...</td>\n",
       "      <td>batch_01_vt/0021.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1 0.16002242 75.58316 78.65596 155.83589 282.6...</td>\n",
       "      <td>batch_01_vt/0028.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0 0.083333336 500.0681 0.0 512.0 0.0 0 0.08333...</td>\n",
       "      <td>batch_01_vt/0031.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1 0.074704476 295.50296 159.4894 366.369 231.2...</td>\n",
       "      <td>batch_01_vt/0032.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1 0.22645006 389.9115 263.38092 494.7198 341.9...</td>\n",
       "      <td>batch_01_vt/0070.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    PredictionString              image_id\n",
       "0  1 0.36776102 343.9898 35.461994 364.5382 46.19...  batch_01_vt/0021.jpg\n",
       "1  1 0.16002242 75.58316 78.65596 155.83589 282.6...  batch_01_vt/0028.jpg\n",
       "2  0 0.083333336 500.0681 0.0 512.0 0.0 0 0.08333...  batch_01_vt/0031.jpg\n",
       "3  1 0.074704476 295.50296 159.4894 366.369 231.2...  batch_01_vt/0032.jpg\n",
       "4  1 0.22645006 389.9115 263.38092 494.7198 341.9...  batch_01_vt/0070.jpg"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg.work_dir = './work_dirs/faster_rcnn_kfold3'\n",
    "model = build_detector(cfg.model, test_cfg=cfg.get('test_cfg'))\n",
    "checkpoint_path = os.path.join(cfg.work_dir, f'epoch_{epoch}.pth')\n",
    "checkpoint = load_checkpoint(model, checkpoint_path, map_location='cpu')\n",
    "\n",
    "model.CLASSES = dataset.CLASSES\n",
    "model = MMDataParallel(model.cuda(), device_ids=[0])\n",
    "output = single_gpu_test(model, data_loader, show_score_thr=0.05)\n",
    "prediction_strings = []\n",
    "file_names = []\n",
    "coco = COCO(cfg.data.test.ann_file)\n",
    "imag_ids = coco.getImgIds()\n",
    "\n",
    "class_num = 11\n",
    "for i, out in enumerate(output):\n",
    "    prediction_string = ''\n",
    "    image_info = coco.loadImgs(coco.getImgIds(imgIds=i))[0]\n",
    "    for j in range(class_num):\n",
    "        for o in out[j]:\n",
    "            prediction_string += str(j) + ' ' + str(o[4]) + ' ' + str(o[0]) + ' ' + str(o[1]) + ' ' + str(\n",
    "                o[2]) + ' ' + str(o[3]) + ' '\n",
    "        \n",
    "    prediction_strings.append(prediction_string)\n",
    "    file_names.append(image_info['file_name'])\n",
    "\n",
    "\n",
    "submission = pd.DataFrame()\n",
    "submission['PredictionString'] = prediction_strings\n",
    "submission['image_id'] = file_names\n",
    "submission.to_csv(os.path.join(cfg.work_dir, f'submission_{epoch}_1x_fold3_.csv'), index=None)\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-16 01:57:27,533 - mmdet - INFO - load model from: torchvision://resnet50\n",
      "2021-05-16 01:57:27,534 - mmdet - INFO - Use load_from_torchvision loader\n",
      "2021-05-16 01:57:27,765 - mmdet - WARNING - The model and loaded state dict do not match exactly\n",
      "\n",
      "unexpected key in source state_dict: fc.weight, fc.bias\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use load_from_local loader\n",
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 837/837, 10.6 task/s, elapsed: 79s, ETA:     0sloading annotations into memory...\n",
      "Done (t=0.15s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PredictionString</th>\n",
       "      <th>image_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1 0.27636898 165.13287 74.08579 254.68384 168....</td>\n",
       "      <td>batch_01_vt/0021.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1 0.056884393 276.7744 247.95915 305.57526 262...</td>\n",
       "      <td>batch_01_vt/0028.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0 0.083333336 498.7401 0.0 512.0 0.0 0 0.08333...</td>\n",
       "      <td>batch_01_vt/0031.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1 0.08946579 295.6789 159.69301 361.1754 231.0...</td>\n",
       "      <td>batch_01_vt/0032.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1 0.23603567 156.19955 299.54343 292.8967 400....</td>\n",
       "      <td>batch_01_vt/0070.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    PredictionString              image_id\n",
       "0  1 0.27636898 165.13287 74.08579 254.68384 168....  batch_01_vt/0021.jpg\n",
       "1  1 0.056884393 276.7744 247.95915 305.57526 262...  batch_01_vt/0028.jpg\n",
       "2  0 0.083333336 498.7401 0.0 512.0 0.0 0 0.08333...  batch_01_vt/0031.jpg\n",
       "3  1 0.08946579 295.6789 159.69301 361.1754 231.0...  batch_01_vt/0032.jpg\n",
       "4  1 0.23603567 156.19955 299.54343 292.8967 400....  batch_01_vt/0070.jpg"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg.work_dir = './work_dirs/faster_rcnn_kfold4'\n",
    "model = build_detector(cfg.model, test_cfg=cfg.get('test_cfg'))\n",
    "checkpoint_path = os.path.join(cfg.work_dir, f'epoch_{epoch}.pth')\n",
    "checkpoint = load_checkpoint(model, checkpoint_path, map_location='cpu')\n",
    "\n",
    "model.CLASSES = dataset.CLASSES\n",
    "model = MMDataParallel(model.cuda(), device_ids=[0])\n",
    "output = single_gpu_test(model, data_loader, show_score_thr=0.05)\n",
    "prediction_strings = []\n",
    "file_names = []\n",
    "coco = COCO(cfg.data.test.ann_file)\n",
    "imag_ids = coco.getImgIds()\n",
    "\n",
    "class_num = 11\n",
    "for i, out in enumerate(output):\n",
    "    prediction_string = ''\n",
    "    image_info = coco.loadImgs(coco.getImgIds(imgIds=i))[0]\n",
    "    for j in range(class_num):\n",
    "        for o in out[j]:\n",
    "            prediction_string += str(j) + ' ' + str(o[4]) + ' ' + str(o[0]) + ' ' + str(o[1]) + ' ' + str(\n",
    "                o[2]) + ' ' + str(o[3]) + ' '\n",
    "        \n",
    "    prediction_strings.append(prediction_string)\n",
    "    file_names.append(image_info['file_name'])\n",
    "\n",
    "\n",
    "submission = pd.DataFrame()\n",
    "submission['PredictionString'] = prediction_strings\n",
    "submission['image_id'] = file_names\n",
    "submission.to_csv(os.path.join(cfg.work_dir, f'submission_{epoch}_1x_fold4_.csv'), index=None)\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.01s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PredictionString</th>\n",
       "      <th>image_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1 0.7233858 163.10098 66.88278 255.18427 164.8...</td>\n",
       "      <td>batch_01_vt/0021.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6 0.12591846 160.01428 154.27768 331.27255 253...</td>\n",
       "      <td>batch_01_vt/0028.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1 0.94246507 61.6754 164.9594 388.50378 391.10...</td>\n",
       "      <td>batch_01_vt/0031.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0 0.08908267 297.62042 161.78137 362.80948 229...</td>\n",
       "      <td>batch_01_vt/0032.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0 0.18186992 39.262924 460.12997 79.42103 473....</td>\n",
       "      <td>batch_01_vt/0070.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    PredictionString              image_id\n",
       "0  1 0.7233858 163.10098 66.88278 255.18427 164.8...  batch_01_vt/0021.jpg\n",
       "1  6 0.12591846 160.01428 154.27768 331.27255 253...  batch_01_vt/0028.jpg\n",
       "2  1 0.94246507 61.6754 164.9594 388.50378 391.10...  batch_01_vt/0031.jpg\n",
       "3  0 0.08908267 297.62042 161.78137 362.80948 229...  batch_01_vt/0032.jpg\n",
       "4  0 0.18186992 39.262924 460.12997 79.42103 473....  batch_01_vt/0070.jpg"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_strings = []\n",
    "file_names = []\n",
    "coco = COCO(cfg.data.test.ann_file)\n",
    "imag_ids = coco.getImgIds()\n",
    "\n",
    "class_num = 11\n",
    "for i, out in enumerate(output):\n",
    "    prediction_string = ''\n",
    "    image_info = coco.loadImgs(coco.getImgIds(imgIds=i))[0]\n",
    "    for j in range(class_num):\n",
    "        for o in out[j]:\n",
    "            prediction_string += str(j) + ' ' + str(o[4]) + ' ' + str(o[0]) + ' ' + str(o[1]) + ' ' + str(\n",
    "                o[2]) + ' ' + str(o[3]) + ' '\n",
    "        \n",
    "    prediction_strings.append(prediction_string)\n",
    "    file_names.append(image_info['file_name'])\n",
    "\n",
    "\n",
    "submission = pd.DataFrame()\n",
    "submission['PredictionString'] = prediction_strings\n",
    "submission['image_id'] = file_names\n",
    "submission.to_csv(os.path.join(cfg.work_dir, f'submission_{epoch}_2x_1333_batch8.csv'), index=None)\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_goal = 2e-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.021025369367535216"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_goal*((1.0001)**500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.02"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_goal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"1\".startswith(\"111\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = \"111\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " \"1\" not in a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def solution(s):\n",
    "    answer = []\n",
    "    for i in s:\n",
    "        start_string = \"\"   # 우선순위 가장 높은 숫자\n",
    "        zzo = \"\"       # 우선순위 두번째 나머지는 i\n",
    "        while True:\n",
    "            if i == \"10\":\n",
    "                start_string += i\n",
    "                i = \"\"\n",
    "                break\n",
    "            if i == \"110\":\n",
    "                zzo += i\n",
    "                i = \"\"\n",
    "                break\n",
    "            if i == \"1\":\n",
    "                zzo += i\n",
    "                i = \"\"\n",
    "                break\n",
    "            if \"0\" not in i:\n",
    "                break\n",
    "            if \"11\" in i:\n",
    "                index = i.index(\"11\")\n",
    "                start_string = start_string + i[:index]\n",
    "                i = i[index:]\n",
    "            else:\n",
    "                start_string = start_string + i\n",
    "                i = \"\"\n",
    "                break\n",
    "            while i.startswith(\"111\"):\n",
    "                if \"0\" not in i:\n",
    "                    break\n",
    "                zzo += \"110\"\n",
    "                index = i.index(\"110\")\n",
    "                i = i[:index] + i[index+3:]\n",
    "                print(i)\n",
    "        answer.append(start_string + zzo + i)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "110\n",
      "111110\n",
      "111\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['1101', '100110110', '0110110111']"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "solution([\"1110\",\"100111100\",\"0111111010\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"11\" < \"110\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def solution(s):\n",
    "    answer = []\n",
    "    for i in s:\n",
    "        one_stack = []\n",
    "        first_s = \"\"\n",
    "        temp = \"\"\n",
    "        zzo_cnt = 0\n",
    "        for j in i:\n",
    "            if j == \"1\":\n",
    "                one_stack.append(\"1\")\n",
    "            else:\n",
    "                if len(one_stack) >= 2:\n",
    "                    one_stack.pop()\n",
    "                    one_stack.pop()\n",
    "                    zzo_cnt += 1\n",
    "                else:\n",
    "                    first_s += \"\".join(one_stack) + \"0\"\n",
    "                    one_stack = []\n",
    "        answer.append(first_s + \"110\"*zzo_cnt + \"\".join(one_stack))\n",
    "    return answer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
